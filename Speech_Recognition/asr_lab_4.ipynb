{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RcyxmRJGqlY"
   },
   "source": [
    "# Практика №4\n",
    "\n",
    "Теперь мы построим и обучим простую end-to-end модель. Будем работать с пропатченной версией уже готового [пайплайна](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch). Также нам пригодится [ESPnet](https://github.com/espnet/espnet) для использования модели [Transformer](http://jalammar.github.io/illustrated-transformer/) в качестве энкодера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDbO_rrWGq7j"
   },
   "source": [
    "### Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4300,
     "status": "ok",
     "timestamp": 1620391417426,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "WzJyomV1JaLp",
    "outputId": "6c21eeb6-3dc5-4de5-b5f6-b0ffa7da384e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchaudio\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 3.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.8.1+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchaudio) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchaudio) (3.7.4.3)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9220,
     "status": "ok",
     "timestamp": 1620391422959,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "TROAsHTXHWik",
    "outputId": "8ab2d2b5-7084-43d1-b695-54a7129b342e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1skrVbNyrhBLeceGS9CV9uIw_gvo1JiA6\n",
      "To: /content/lab4.zip\n",
      "\r",
      "0.00B [00:00, ?B/s]\r",
      "2.77MB [00:00, 87.3MB/s]\n",
      "/content/lab4\n"
     ]
    }
   ],
   "source": [
    "!gdown --id '1skrVbNyrhBLeceGS9CV9uIw_gvo1JiA6'\n",
    "\n",
    "!unzip -q lab4.zip\n",
    "!rm -rf lab4.zip sample_data\n",
    "%cd lab4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10283,
     "status": "ok",
     "timestamp": 1620391425808,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "m4wcCtkIH2dn"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from utils import TextTransform\n",
    "from utils import cer\n",
    "from utils import wer\n",
    "\n",
    "from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n",
    "from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\n",
    "from espnet.nets.pytorch_backend.transformer.repeat import repeat\n",
    "from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n",
    "from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import PositionwiseFeedForward\n",
    "from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n",
    "from espnet.nets.pytorch_backend.nets_utils import make_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1246,
     "status": "ok",
     "timestamp": 1620391429782,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "NaESUZiHJgfN"
   },
   "outputs": [],
   "source": [
    "train_audio_transforms = torch.nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000,\n",
    "                                                              n_fft=400,\n",
    "                                                              hop_length=160,\n",
    "                                                              n_mels=80)\n",
    "\n",
    "text_transform = TextTransform()\n",
    "\n",
    "#-----------------------------TODO №2-----------------------------------\n",
    "# Заменить графемный токенайзер на сабвордовый TextTransformBPE\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0])\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "        decode = []\n",
    "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "        for j, index in enumerate(args):\n",
    "            if index != blank_label:\n",
    "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "                    continue\n",
    "                decode.append(index.item())\n",
    "        decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1311,
     "status": "ok",
     "timestamp": 1620391434224,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "9OqoVLnrJsCV"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=80,\n",
    "        output_size=29,\n",
    "        conv2d_filters=32,\n",
    "        attention_dim=360,\n",
    "        attention_heads=8,\n",
    "        feedforward_dim=1024,\n",
    "        num_layers=10,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.conv_in = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(conv2d_filters, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.conv_out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(conv2d_filters * ((input_size // 2) // 2), attention_dim),\n",
    "            PositionalEncoding(attention_dim, 0.1),\n",
    "        )\n",
    "        positionwise_layer = PositionwiseFeedForward\n",
    "        positionwise_layer_args = (attention_dim, feedforward_dim, dropout)\n",
    "        self.encoder_layer = repeat(\n",
    "            num_layers,\n",
    "            lambda lnum: EncoderLayer(\n",
    "                attention_dim,\n",
    "                MultiHeadedAttention(\n",
    "                    attention_heads, attention_dim, dropout\n",
    "                ),\n",
    "                positionwise_layer(*positionwise_layer_args),\n",
    "                dropout,\n",
    "                normalize_before=True,\n",
    "                concat_after=False,\n",
    "            ),\n",
    "        )\n",
    "        self.after_norm = LayerNorm(attention_dim)\n",
    "        self.final_layer = torch.nn.Linear(attention_dim, output_size)\n",
    "\n",
    "    def forward(self, x, ilens):\n",
    "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
    "        x = self.conv_in(x)\n",
    "        b, c, t, f = x.size()\n",
    "        x = self.conv_out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
    "        masks = (~make_pad_mask(ilens)[:, None, :])[:, :, ::4].to(x.device)\n",
    "        x, _ = self.encoder_layer(x, masks)\n",
    "        x = self.after_norm(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1344,
     "status": "ok",
     "timestamp": 1620391436766,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "d2p_8IjeKkqq"
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data \n",
    "        spectrograms, labels = spectrograms[:, :, :,:max(input_lengths)].to(device), labels.to(device) #(batch, 1, feat_dim, time)\n",
    "        spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch, time, feat_dim,)\n",
    "        # print(labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(spectrograms, input_lengths)  # (batch, time, n_classes)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "        input_lengths = [x // 4 for x in input_lengths]\n",
    "\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                100. * batch_idx / len(train_loader), loss.item(), scheduler.get_last_lr()[0]))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch time, feat_dim,)\n",
    "            \n",
    "            output = model(spectrograms, input_lengths)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "            input_lengths = [x // 4 for x in input_lengths]\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j], ignore_case=True))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j], ignore_case=True))\n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1279,
     "status": "ok",
     "timestamp": 1620391438283,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "MzEbtsB1LKsh"
   },
   "outputs": [],
   "source": [
    "def main(learning_rate=1e-5, batch_size=20, test_batch_size=7, epochs=10,\n",
    "        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
    "    \n",
    "    hparams = {\n",
    "        \"input_size\": 80,\n",
    "        \"output_size\": 29,\n",
    "        \"conv2d_filters\": 32,\n",
    "        \"attention_dim\": 360,\n",
    "        \"attention_heads\": 8,\n",
    "        \"feedforward_dim\": 1024,\n",
    "        \"num_layers\":10,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=test_batch_size,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "    model = TransformerModel(\n",
    "        hparams['input_size'],\n",
    "        hparams['output_size'],\n",
    "        hparams['conv2d_filters'],\n",
    "        hparams['attention_dim'],\n",
    "        hparams['attention_heads'],\n",
    "        hparams['feedforward_dim'],\n",
    "        hparams['num_layers'],\n",
    "        hparams['dropout']).to(device)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = torch.nn.CTCLoss(blank=28, zero_infinity=False).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        !date\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "        test(model, device, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9478,
     "status": "error",
     "timestamp": 1620306682504,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "eExZLsUiLeXk",
    "outputId": "d66d7269-b1b2-48c4-83d6-519584f285f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (conv_in): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_out): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=360, bias=True)\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer): MultiSequential(\n",
      "    (0): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (1): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (2): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (3): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (4): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (5): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (6): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (7): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (8): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (9): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (after_norm): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "  (final_layer): Linear(in_features=360, out_features=29, bias=True)\n",
      ")\n",
      "Num Model Parameters 12850957\n",
      "Thu May  6 13:11:13 UTC 2021\n",
      "Train Epoch: 1 [0/28539 (0%)]\tLoss: 4.994817\tLR: 0.000040\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-332bda6d8704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlibri_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test-clean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-97-a1086a7a79e6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(learning_rate, batch_size, test_batch_size, epochs, train_url, test_url)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-bd703d330572>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, time, n_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (time, batch, n_class)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cbcd47496297>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, ilens)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmake_pad_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0milens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/lab4/espnet/nets/pytorch_backend/transformer/repeat.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;34m\"\"\"Repeat.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/lab4/espnet/nets/pytorch_backend/transformer/encoder_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, cache)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_concat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/lab4/espnet/nets/pytorch_backend/transformer/attention.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/lab4/espnet/nets/pytorch_backend/transformer/attention.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \"\"\"\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_qkv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/lab4/espnet/nets/pytorch_backend/transformer/attention.py\u001b[0m in \u001b[0;36mforward_qkv\u001b[0;34m(self, query, key, value)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \"\"\"\n\u001b[1;32m     61\u001b[0m         \u001b[0mn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    872\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    873\u001b[0m                 \u001b[0m_global_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m                 self._forward_pre_hooks.values()):\n\u001b[0m\u001b[1;32m    875\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 10\n",
    "test_batch_size = 7\n",
    "epochs = 10\n",
    "libri_train_set = \"train-clean-100\"\n",
    "libri_test_set = \"test-clean\"\n",
    "\n",
    "main(learning_rate, batch_size, test_batch_size, epochs, libri_train_set, libri_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mby39YVqZadd"
   },
   "source": [
    "### <b>Задание №1</b> (5 баллов):\n",
    "На данный момент практически все E2E SOTA решения использую [сабворды](https://dyakonov.org/2019/11/29/%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%BD%D0%B0-%D0%BF%D0%BE%D0%B4%D1%81%D0%BB%D0%BE%D0%B2%D0%B0-subword-tokenization/) (subwords/wordpieces) в качестве таргетов нейронки для распознавания. Нам бы тоже не мешало перейти от графем к сабвордам. Теперь вместо букв (графем) будем распознавать кусочки слов. В качестве такого токенайзера предлагается использовать [Sentencepiece](https://github.com/google/sentencepiece). Главное правильно обернуть его в наш класс TextTransform. Текстовый файл (train_clean_100_text_clean.txt) для обучения токенайзера уже подготовлен и лежит в корневой папке проекта. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3497,
     "status": "ok",
     "timestamp": 1620391448132,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "6sVeBBhjpfzx",
    "outputId": "3584ca5a-0c20-4c00-8b9a-88e10353277d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 24.0MB/s eta 0:00:01\r",
      "\u001b[K     |▌                               | 20kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |▉                               | 30kB 5.3MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 40kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 51kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |█▋                              | 61kB 3.2MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 71kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 81kB 3.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▌                             | 92kB 3.7MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 102kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 112kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▎                            | 122kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▌                            | 133kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▉                            | 143kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 153kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▍                           | 163kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 174kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████                           | 184kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 194kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 204kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▊                          | 215kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 225kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 235kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▌                         | 245kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████▉                         | 256kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 266kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 276kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 286kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 296kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▏                       | 307kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 317kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 327kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 337kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 348kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 358kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 368kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 378kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 389kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 399kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 409kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 419kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 430kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 440kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 450kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▎                   | 460kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 471kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 481kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 491kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 501kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▋                  | 512kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▉                  | 522kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 532kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 542kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▊                 | 552kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 563kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 573kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 583kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 593kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 604kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 614kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▋               | 624kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 634kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▏              | 645kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 655kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 665kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 675kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 686kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▌             | 696kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 706kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 716kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 727kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 737kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 747kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 757kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 768kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 778kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 788kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 798kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 808kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▉          | 819kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 829kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 839kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 849kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 860kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 870kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 880kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▊        | 890kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 901kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 911kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▌       | 921kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 931kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 942kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 952kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 962kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▉      | 972kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 983kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 993kB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▊     | 1.0MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 1.0MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▎    | 1.0MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 1.0MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 1.0MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 1.1MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 1.1MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▋   | 1.1MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 1.1MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▏  | 1.1MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 1.1MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 1.1MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.1MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 1.1MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.1MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▊ | 1.2MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 1.2MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▎| 1.2MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 1.2MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▉| 1.2MB 3.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.2MB 3.8MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.95\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3029,
     "status": "ok",
     "timestamp": 1620391448134,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "7e9J_CAIqHme"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2336,
     "status": "ok",
     "timestamp": 1620391448136,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "hPOmdQ9Rpilc"
   },
   "outputs": [],
   "source": [
    "NUM_UNITS = 4000\n",
    "BLANK = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1258,
     "status": "ok",
     "timestamp": 1620391448137,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "JNbiW919e2le"
   },
   "outputs": [],
   "source": [
    "class TextTransformBPE:\n",
    "    def __init__(self, train_text, num_units=NUM_UNITS):\n",
    "        spm.SentencePieceTrainer.train(input=train_text,\n",
    "                                       model_prefix='m',\n",
    "                                       vocab_size=NUM_UNITS,\n",
    "                                       model_type='bpe')\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load('m.model')\n",
    "        \"\"\" Обучение BPE модели на 4000 юнитов\"\"\"\n",
    "        return\n",
    "        # pass\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Преобразование входного текста в последовательность сабвордов в формате их индекса в BPE модели \"\"\"\n",
    "        int_sequence = self.sp.encode(text)\n",
    "        # pass\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Преобразование последовательности индексов сабвордов в текст \"\"\"\n",
    "        labels = [int(x) for x in labels]\n",
    "        # print(labels)\n",
    "        string = self.sp.decode(labels)\n",
    "        # pass\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2353,
     "status": "ok",
     "timestamp": 1620391451309,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "8k4cpWFdqy8E"
   },
   "outputs": [],
   "source": [
    "tbpe = TextTransformBPE('train_clean_100_text_clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1561,
     "status": "ok",
     "timestamp": 1620391451311,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "Tm6b0kIarCS_",
    "outputId": "adb24013-5a1c-4982-f54a-75b0eab0ccca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3973, 0]"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbpe.text_to_int(' Z ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 43
    },
    "executionInfo": {
     "elapsed": 726,
     "status": "ok",
     "timestamp": 1620391452151,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "kCSfswlFsUpj",
    "outputId": "f4259853-fccb-48c6-e55f-21c72d0d1dbb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'BALL'"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbpe.int_to_text([1998])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1620391454681,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "5HTrHn5cwUGS",
    "outputId": "b19d24b1-869d-4f8e-ff39-ac839a1d5b21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbpe.text_to_int('O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 574,
     "status": "ok",
     "timestamp": 1620391456302,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "6WrxIdspx60l"
   },
   "outputs": [],
   "source": [
    "def test(model, device, test_loader, criterion, epoch):\n",
    "    print('\\nevaluating...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_cer, test_wer = [], []\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch time, feat_dim,)\n",
    "            \n",
    "            output = model(spectrograms, input_lengths)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "            input_lengths = [x // 4 for x in input_lengths]\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            # print(output, labels, label_lengths)\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths, blank_label=BLANK)\n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 1135,
     "status": "ok",
     "timestamp": 1620391460110,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "3ISNbe_wuSDp"
   },
   "outputs": [],
   "source": [
    "def main(learning_rate=1e-5, batch_size=20, test_batch_size=7, epochs=10,\n",
    "        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
    "    \n",
    "    hparams = {\n",
    "        \"input_size\": 80,\n",
    "        \"output_size\": NUM_UNITS + 1,\n",
    "        \"conv2d_filters\": 32,\n",
    "        \"attention_dim\": 360,\n",
    "        \"attention_heads\": 8,\n",
    "        \"feedforward_dim\": 1024,\n",
    "        \"num_layers\":10,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=test_batch_size,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "    model = TransformerModel(\n",
    "        hparams['input_size'],\n",
    "        hparams['output_size'],\n",
    "        hparams['conv2d_filters'],\n",
    "        hparams['attention_dim'],\n",
    "        hparams['attention_heads'],\n",
    "        hparams['feedforward_dim'],\n",
    "        hparams['num_layers'],\n",
    "        hparams['dropout']).to(device)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = torch.nn.CTCLoss(blank=BLANK, zero_infinity=False).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        !date\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "        test(model, device, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 1305,
     "status": "ok",
     "timestamp": 1620391461561,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "SEWAUifX77zV"
   },
   "outputs": [],
   "source": [
    "def data_processing(data, data_type=\"train\"):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        if data_type == 'train':\n",
    "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        elif data_type == 'valid':\n",
    "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
    "        else:\n",
    "            raise Exception('data_type should be train or valid')\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.upper()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0])\n",
    "        label_lengths.append(len(label))\n",
    "\n",
    "    spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 2425,
     "status": "ok",
     "timestamp": 1620391464093,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "qHCYM1cZh0JS"
   },
   "outputs": [],
   "source": [
    "text_transform = TextTransformBPE('train_clean_100_text_clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "188c8e41fb8b43cc8931193271d300be",
      "531501918e52470dbc06ed1bcd91f547",
      "dbf84fb63b8c4b438cef90b4328a4502",
      "53064c0e94134dddba7dac19c24a6b6c",
      "c92aacae1e5449c5a0d9ceea7ce0e01b",
      "df1c0693de1f4d8bbb8bd5a67be639fd",
      "d6d9a6a758244ce9a2b5341d584cae80",
      "133f24e923fe4dd29fce17737471b515",
      "d3a6f08599274787ae924cf8d011e3e2",
      "bc684a77a490426e9758d117cefe722c",
      "964f1d0b0efa47b2ab21fd338e989409",
      "90bab181629a417fbe73ed2f547bbe9f",
      "7070e1b844c44b52a0ff5c748d0eaf61",
      "9bc762a242d648ddba060f04929563fc",
      "5ed8eba52718496da1c9bfd1ce765cb0",
      "89f11cc02fc044b2a37ab6c0303dbcda"
     ]
    },
    "executionInfo": {
     "elapsed": 565580,
     "status": "ok",
     "timestamp": 1620367533647,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "DG3DBPuhyICe",
    "outputId": "074278c9-86ac-44b0-fac8-312629df88d5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188c8e41fb8b43cc8931193271d300be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6387309499.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a6f08599274787ae924cf8d011e3e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=346663984.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TransformerModel(\n",
      "  (conv_in): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_out): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=360, bias=True)\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer): MultiSequential(\n",
      "    (0): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (1): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (2): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (3): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (4): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (5): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (6): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (7): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (8): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (9): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (after_norm): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "  (final_layer): Linear(in_features=360, out_features=4001, bias=True)\n",
      ")\n",
      "Num Model Parameters 14284849\n",
      "Fri May  7 04:02:26 UTC 2021\n",
      "Train Epoch: 1 [0/28539 (0%)]\tLoss: 52.891735\tLR: 0.000040\n",
      "Train Epoch: 1 [1000/28539 (4%)]\tLoss: 7.227303\tLR: 0.000051\n",
      "Train Epoch: 1 [2000/28539 (7%)]\tLoss: 6.890680\tLR: 0.000063\n",
      "Train Epoch: 1 [3000/28539 (11%)]\tLoss: 6.856912\tLR: 0.000074\n",
      "Train Epoch: 1 [4000/28539 (14%)]\tLoss: 6.955190\tLR: 0.000085\n",
      "Train Epoch: 1 [5000/28539 (18%)]\tLoss: 6.868718\tLR: 0.000096\n",
      "Train Epoch: 1 [6000/28539 (21%)]\tLoss: 6.867096\tLR: 0.000107\n",
      "Train Epoch: 1 [7000/28539 (25%)]\tLoss: 6.973543\tLR: 0.000119\n",
      "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 7.072525\tLR: 0.000130\n",
      "Train Epoch: 1 [9000/28539 (32%)]\tLoss: 6.853316\tLR: 0.000141\n",
      "Train Epoch: 1 [10000/28539 (35%)]\tLoss: 6.928628\tLR: 0.000152\n",
      "Train Epoch: 1 [11000/28539 (39%)]\tLoss: 6.932291\tLR: 0.000163\n",
      "Train Epoch: 1 [12000/28539 (42%)]\tLoss: 6.891555\tLR: 0.000175\n",
      "Train Epoch: 1 [13000/28539 (46%)]\tLoss: 6.844495\tLR: 0.000186\n",
      "Train Epoch: 1 [14000/28539 (49%)]\tLoss: 6.939804\tLR: 0.000197\n",
      "Train Epoch: 1 [15000/28539 (53%)]\tLoss: 6.769685\tLR: 0.000208\n",
      "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 6.866283\tLR: 0.000220\n",
      "Train Epoch: 1 [17000/28539 (60%)]\tLoss: 6.840684\tLR: 0.000231\n",
      "Train Epoch: 1 [18000/28539 (63%)]\tLoss: 6.979131\tLR: 0.000242\n",
      "Train Epoch: 1 [19000/28539 (67%)]\tLoss: 7.041498\tLR: 0.000253\n",
      "Train Epoch: 1 [20000/28539 (70%)]\tLoss: 6.754084\tLR: 0.000264\n",
      "Train Epoch: 1 [21000/28539 (74%)]\tLoss: 6.785573\tLR: 0.000276\n",
      "Train Epoch: 1 [22000/28539 (77%)]\tLoss: 6.942201\tLR: 0.000287\n",
      "Train Epoch: 1 [23000/28539 (81%)]\tLoss: 6.594897\tLR: 0.000298\n",
      "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 6.634675\tLR: 0.000309\n",
      "Train Epoch: 1 [25000/28539 (88%)]\tLoss: 6.635414\tLR: 0.000320\n",
      "Train Epoch: 1 [26000/28539 (91%)]\tLoss: 6.756499\tLR: 0.000332\n",
      "Train Epoch: 1 [27000/28539 (95%)]\tLoss: 6.717818\tLR: 0.000343\n",
      "Train Epoch: 1 [28000/28539 (98%)]\tLoss: 6.680356\tLR: 0.000354\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 6.6359, Average CER: 0.996080 Average WER: 0.9979\n",
      "\n",
      "Fri May  7 04:11:19 UTC 2021\n",
      "Train Epoch: 2 [0/28539 (0%)]\tLoss: 6.590364\tLR: 0.000360\n",
      "Train Epoch: 2 [1000/28539 (4%)]\tLoss: 6.651708\tLR: 0.000371\n",
      "Train Epoch: 2 [2000/28539 (7%)]\tLoss: 6.606496\tLR: 0.000383\n",
      "Train Epoch: 2 [3000/28539 (11%)]\tLoss: 6.340874\tLR: 0.000394\n",
      "Train Epoch: 2 [4000/28539 (14%)]\tLoss: 6.242527\tLR: 0.000405\n",
      "Train Epoch: 2 [5000/28539 (18%)]\tLoss: 6.319694\tLR: 0.000416\n",
      "Train Epoch: 2 [6000/28539 (21%)]\tLoss: 6.439886\tLR: 0.000427\n",
      "Train Epoch: 2 [7000/28539 (25%)]\tLoss: 6.398027\tLR: 0.000439\n",
      "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 6.494522\tLR: 0.000450\n",
      "Train Epoch: 2 [9000/28539 (32%)]\tLoss: 5.655443\tLR: 0.000461\n",
      "Train Epoch: 2 [10000/28539 (35%)]\tLoss: 6.278307\tLR: 0.000472\n",
      "Train Epoch: 2 [11000/28539 (39%)]\tLoss: 6.085635\tLR: 0.000483\n",
      "Train Epoch: 2 [12000/28539 (42%)]\tLoss: 6.086195\tLR: 0.000495\n",
      "Train Epoch: 2 [13000/28539 (46%)]\tLoss: 6.074592\tLR: 0.000506\n",
      "Train Epoch: 2 [14000/28539 (49%)]\tLoss: 5.722183\tLR: 0.000517\n",
      "Train Epoch: 2 [15000/28539 (53%)]\tLoss: 5.943208\tLR: 0.000528\n",
      "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 5.552375\tLR: 0.000540\n",
      "Train Epoch: 2 [17000/28539 (60%)]\tLoss: 5.558404\tLR: 0.000551\n",
      "Train Epoch: 2 [18000/28539 (63%)]\tLoss: 5.833836\tLR: 0.000562\n",
      "Train Epoch: 2 [19000/28539 (67%)]\tLoss: 5.496469\tLR: 0.000573\n",
      "Train Epoch: 2 [20000/28539 (70%)]\tLoss: 5.705594\tLR: 0.000584\n",
      "Train Epoch: 2 [21000/28539 (74%)]\tLoss: 5.638604\tLR: 0.000596\n",
      "Train Epoch: 2 [22000/28539 (77%)]\tLoss: 5.378339\tLR: 0.000607\n",
      "Train Epoch: 2 [23000/28539 (81%)]\tLoss: 5.281610\tLR: 0.000618\n",
      "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 5.261878\tLR: 0.000629\n",
      "Train Epoch: 2 [25000/28539 (88%)]\tLoss: 4.903787\tLR: 0.000640\n",
      "Train Epoch: 2 [26000/28539 (91%)]\tLoss: 5.404764\tLR: 0.000652\n",
      "Train Epoch: 2 [27000/28539 (95%)]\tLoss: 5.325154\tLR: 0.000663\n",
      "Train Epoch: 2 [28000/28539 (98%)]\tLoss: 5.121409\tLR: 0.000674\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 4.9144, Average CER: 0.761731 Average WER: 0.8674\n",
      "\n",
      "Fri May  7 04:21:51 UTC 2021\n",
      "Train Epoch: 3 [0/28539 (0%)]\tLoss: 5.107850\tLR: 0.000680\n",
      "Train Epoch: 3 [1000/28539 (4%)]\tLoss: 5.420509\tLR: 0.000691\n",
      "Train Epoch: 3 [2000/28539 (7%)]\tLoss: 5.034051\tLR: 0.000703\n",
      "Train Epoch: 3 [3000/28539 (11%)]\tLoss: 5.422525\tLR: 0.000714\n",
      "Train Epoch: 3 [4000/28539 (14%)]\tLoss: 5.332728\tLR: 0.000725\n",
      "Train Epoch: 3 [5000/28539 (18%)]\tLoss: 4.981529\tLR: 0.000736\n",
      "Train Epoch: 3 [6000/28539 (21%)]\tLoss: 5.166149\tLR: 0.000747\n",
      "Train Epoch: 3 [7000/28539 (25%)]\tLoss: 4.609148\tLR: 0.000759\n",
      "Train Epoch: 3 [8000/28539 (28%)]\tLoss: 5.008317\tLR: 0.000770\n",
      "Train Epoch: 3 [9000/28539 (32%)]\tLoss: 5.158212\tLR: 0.000781\n",
      "Train Epoch: 3 [10000/28539 (35%)]\tLoss: 4.959920\tLR: 0.000792\n",
      "Train Epoch: 3 [11000/28539 (39%)]\tLoss: 5.088552\tLR: 0.000804\n",
      "Train Epoch: 3 [12000/28539 (42%)]\tLoss: 4.801769\tLR: 0.000815\n",
      "Train Epoch: 3 [13000/28539 (46%)]\tLoss: 4.861115\tLR: 0.000826\n",
      "Train Epoch: 3 [14000/28539 (49%)]\tLoss: 4.903052\tLR: 0.000837\n",
      "Train Epoch: 3 [15000/28539 (53%)]\tLoss: 5.238155\tLR: 0.000848\n",
      "Train Epoch: 3 [16000/28539 (56%)]\tLoss: 5.043576\tLR: 0.000860\n",
      "Train Epoch: 3 [17000/28539 (60%)]\tLoss: 4.384335\tLR: 0.000871\n",
      "Train Epoch: 3 [18000/28539 (63%)]\tLoss: 4.785975\tLR: 0.000882\n",
      "Train Epoch: 3 [19000/28539 (67%)]\tLoss: 4.968513\tLR: 0.000893\n",
      "Train Epoch: 3 [20000/28539 (70%)]\tLoss: 4.763910\tLR: 0.000904\n",
      "Train Epoch: 3 [21000/28539 (74%)]\tLoss: 4.662384\tLR: 0.000916\n",
      "Train Epoch: 3 [22000/28539 (77%)]\tLoss: 4.740371\tLR: 0.000927\n",
      "Train Epoch: 3 [23000/28539 (81%)]\tLoss: 4.809920\tLR: 0.000938\n",
      "Train Epoch: 3 [24000/28539 (84%)]\tLoss: 4.792778\tLR: 0.000949\n",
      "Train Epoch: 3 [25000/28539 (88%)]\tLoss: 4.733207\tLR: 0.000961\n",
      "Train Epoch: 3 [26000/28539 (91%)]\tLoss: 4.598130\tLR: 0.000972\n",
      "Train Epoch: 3 [27000/28539 (95%)]\tLoss: 4.947113\tLR: 0.000983\n",
      "Train Epoch: 3 [28000/28539 (98%)]\tLoss: 4.728830\tLR: 0.000994\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 4.3125, Average CER: 0.656438 Average WER: 0.8021\n",
      "\n",
      "Fri May  7 04:33:15 UTC 2021\n",
      "Train Epoch: 4 [0/28539 (0%)]\tLoss: 4.580566\tLR: 0.001000\n",
      "Train Epoch: 4 [1000/28539 (4%)]\tLoss: 4.836166\tLR: 0.000995\n",
      "Train Epoch: 4 [2000/28539 (7%)]\tLoss: 4.611443\tLR: 0.000990\n",
      "Train Epoch: 4 [3000/28539 (11%)]\tLoss: 4.670081\tLR: 0.000985\n",
      "Train Epoch: 4 [4000/28539 (14%)]\tLoss: 4.550425\tLR: 0.000980\n",
      "Train Epoch: 4 [5000/28539 (18%)]\tLoss: 4.625463\tLR: 0.000975\n",
      "Train Epoch: 4 [6000/28539 (21%)]\tLoss: 4.603614\tLR: 0.000970\n",
      "Train Epoch: 4 [7000/28539 (25%)]\tLoss: 4.800008\tLR: 0.000965\n",
      "Train Epoch: 4 [8000/28539 (28%)]\tLoss: 4.035579\tLR: 0.000960\n",
      "Train Epoch: 4 [9000/28539 (32%)]\tLoss: 4.782927\tLR: 0.000955\n",
      "Train Epoch: 4 [10000/28539 (35%)]\tLoss: 4.662711\tLR: 0.000950\n",
      "Train Epoch: 4 [11000/28539 (39%)]\tLoss: 4.735173\tLR: 0.000945\n",
      "Train Epoch: 4 [12000/28539 (42%)]\tLoss: 4.541592\tLR: 0.000940\n",
      "Train Epoch: 4 [13000/28539 (46%)]\tLoss: 4.521434\tLR: 0.000935\n",
      "Train Epoch: 4 [14000/28539 (49%)]\tLoss: 4.917145\tLR: 0.000930\n",
      "Train Epoch: 4 [15000/28539 (53%)]\tLoss: 4.671381\tLR: 0.000925\n",
      "Train Epoch: 4 [16000/28539 (56%)]\tLoss: 4.520216\tLR: 0.000920\n",
      "Train Epoch: 4 [17000/28539 (60%)]\tLoss: 4.300691\tLR: 0.000915\n",
      "Train Epoch: 4 [18000/28539 (63%)]\tLoss: 4.000021\tLR: 0.000910\n",
      "Train Epoch: 4 [19000/28539 (67%)]\tLoss: 4.513920\tLR: 0.000905\n",
      "Train Epoch: 4 [20000/28539 (70%)]\tLoss: 4.283582\tLR: 0.000900\n",
      "Train Epoch: 4 [21000/28539 (74%)]\tLoss: 4.331577\tLR: 0.000895\n",
      "Train Epoch: 4 [22000/28539 (77%)]\tLoss: 4.182439\tLR: 0.000890\n",
      "Train Epoch: 4 [23000/28539 (81%)]\tLoss: 4.123166\tLR: 0.000885\n",
      "Train Epoch: 4 [24000/28539 (84%)]\tLoss: 4.069209\tLR: 0.000880\n",
      "Train Epoch: 4 [25000/28539 (88%)]\tLoss: 4.479592\tLR: 0.000875\n",
      "Train Epoch: 4 [26000/28539 (91%)]\tLoss: 4.331651\tLR: 0.000870\n",
      "Train Epoch: 4 [27000/28539 (95%)]\tLoss: 4.103443\tLR: 0.000865\n",
      "Train Epoch: 4 [28000/28539 (98%)]\tLoss: 4.030522\tLR: 0.000860\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.6660, Average CER: 0.551531 Average WER: 0.7218\n",
      "\n",
      "Fri May  7 04:45:31 UTC 2021\n",
      "Train Epoch: 5 [0/28539 (0%)]\tLoss: 3.825006\tLR: 0.000857\n",
      "Train Epoch: 5 [1000/28539 (4%)]\tLoss: 4.206026\tLR: 0.000852\n",
      "Train Epoch: 5 [2000/28539 (7%)]\tLoss: 3.941730\tLR: 0.000847\n",
      "Train Epoch: 5 [3000/28539 (11%)]\tLoss: 3.953220\tLR: 0.000842\n",
      "Train Epoch: 5 [4000/28539 (14%)]\tLoss: 3.870993\tLR: 0.000837\n",
      "Train Epoch: 5 [5000/28539 (18%)]\tLoss: 3.784168\tLR: 0.000832\n",
      "Train Epoch: 5 [6000/28539 (21%)]\tLoss: 3.781160\tLR: 0.000827\n",
      "Train Epoch: 5 [7000/28539 (25%)]\tLoss: 3.754234\tLR: 0.000822\n",
      "Train Epoch: 5 [8000/28539 (28%)]\tLoss: 3.977174\tLR: 0.000817\n",
      "Train Epoch: 5 [9000/28539 (32%)]\tLoss: 3.993175\tLR: 0.000812\n",
      "Train Epoch: 5 [10000/28539 (35%)]\tLoss: 4.088288\tLR: 0.000807\n",
      "Train Epoch: 5 [11000/28539 (39%)]\tLoss: 3.814224\tLR: 0.000802\n",
      "Train Epoch: 5 [12000/28539 (42%)]\tLoss: 3.454484\tLR: 0.000797\n",
      "Train Epoch: 5 [13000/28539 (46%)]\tLoss: 3.955325\tLR: 0.000792\n",
      "Train Epoch: 5 [14000/28539 (49%)]\tLoss: 3.780366\tLR: 0.000787\n",
      "Train Epoch: 5 [15000/28539 (53%)]\tLoss: 3.760143\tLR: 0.000782\n",
      "Train Epoch: 5 [16000/28539 (56%)]\tLoss: 3.499926\tLR: 0.000777\n",
      "Train Epoch: 5 [17000/28539 (60%)]\tLoss: 3.561694\tLR: 0.000772\n",
      "Train Epoch: 5 [18000/28539 (63%)]\tLoss: 3.596829\tLR: 0.000767\n",
      "Train Epoch: 5 [19000/28539 (67%)]\tLoss: 4.062776\tLR: 0.000762\n",
      "Train Epoch: 5 [20000/28539 (70%)]\tLoss: 3.547837\tLR: 0.000757\n",
      "Train Epoch: 5 [21000/28539 (74%)]\tLoss: 4.102931\tLR: 0.000752\n",
      "Train Epoch: 5 [22000/28539 (77%)]\tLoss: 3.706166\tLR: 0.000747\n",
      "Train Epoch: 5 [23000/28539 (81%)]\tLoss: 3.589657\tLR: 0.000742\n",
      "Train Epoch: 5 [24000/28539 (84%)]\tLoss: 3.387999\tLR: 0.000737\n",
      "Train Epoch: 5 [25000/28539 (88%)]\tLoss: 3.617470\tLR: 0.000732\n",
      "Train Epoch: 5 [26000/28539 (91%)]\tLoss: 3.323829\tLR: 0.000727\n",
      "Train Epoch: 5 [27000/28539 (95%)]\tLoss: 3.638919\tLR: 0.000722\n",
      "Train Epoch: 5 [28000/28539 (98%)]\tLoss: 3.377362\tLR: 0.000717\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.0976, Average CER: 0.450533 Average WER: 0.6428\n",
      "\n",
      "Fri May  7 04:58:31 UTC 2021\n",
      "Train Epoch: 6 [0/28539 (0%)]\tLoss: 3.200869\tLR: 0.000714\n",
      "Train Epoch: 6 [1000/28539 (4%)]\tLoss: 3.310806\tLR: 0.000709\n",
      "Train Epoch: 6 [2000/28539 (7%)]\tLoss: 3.423640\tLR: 0.000704\n",
      "Train Epoch: 6 [3000/28539 (11%)]\tLoss: 3.696031\tLR: 0.000699\n",
      "Train Epoch: 6 [4000/28539 (14%)]\tLoss: 3.447461\tLR: 0.000694\n",
      "Train Epoch: 6 [5000/28539 (18%)]\tLoss: 3.473422\tLR: 0.000689\n",
      "Train Epoch: 6 [6000/28539 (21%)]\tLoss: 3.261634\tLR: 0.000684\n",
      "Train Epoch: 6 [7000/28539 (25%)]\tLoss: 3.501712\tLR: 0.000679\n",
      "Train Epoch: 6 [8000/28539 (28%)]\tLoss: 3.299363\tLR: 0.000674\n",
      "Train Epoch: 6 [9000/28539 (32%)]\tLoss: 3.278606\tLR: 0.000669\n",
      "Train Epoch: 6 [10000/28539 (35%)]\tLoss: 3.622344\tLR: 0.000664\n",
      "Train Epoch: 6 [11000/28539 (39%)]\tLoss: 3.402110\tLR: 0.000659\n",
      "Train Epoch: 6 [12000/28539 (42%)]\tLoss: 3.602710\tLR: 0.000654\n",
      "Train Epoch: 6 [13000/28539 (46%)]\tLoss: 3.352204\tLR: 0.000649\n",
      "Train Epoch: 6 [14000/28539 (49%)]\tLoss: 3.742323\tLR: 0.000644\n",
      "Train Epoch: 6 [15000/28539 (53%)]\tLoss: 3.302977\tLR: 0.000639\n",
      "Train Epoch: 6 [16000/28539 (56%)]\tLoss: 3.425740\tLR: 0.000634\n",
      "Train Epoch: 6 [17000/28539 (60%)]\tLoss: 4.037282\tLR: 0.000629\n",
      "Train Epoch: 6 [18000/28539 (63%)]\tLoss: 3.130060\tLR: 0.000624\n",
      "Train Epoch: 6 [19000/28539 (67%)]\tLoss: 3.229598\tLR: 0.000619\n",
      "Train Epoch: 6 [20000/28539 (70%)]\tLoss: 3.676013\tLR: 0.000614\n",
      "Train Epoch: 6 [21000/28539 (74%)]\tLoss: 3.176782\tLR: 0.000609\n",
      "Train Epoch: 6 [22000/28539 (77%)]\tLoss: 3.068876\tLR: 0.000604\n",
      "Train Epoch: 6 [23000/28539 (81%)]\tLoss: 3.609450\tLR: 0.000599\n",
      "Train Epoch: 6 [24000/28539 (84%)]\tLoss: 3.102463\tLR: 0.000594\n",
      "Train Epoch: 6 [25000/28539 (88%)]\tLoss: 3.064046\tLR: 0.000589\n",
      "Train Epoch: 6 [26000/28539 (91%)]\tLoss: 3.275744\tLR: 0.000584\n",
      "Train Epoch: 6 [27000/28539 (95%)]\tLoss: 3.448975\tLR: 0.000579\n",
      "Train Epoch: 6 [28000/28539 (98%)]\tLoss: 3.176452\tLR: 0.000574\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.7201, Average CER: 0.397463 Average WER: 0.5939\n",
      "\n",
      "Fri May  7 05:11:45 UTC 2021\n",
      "Train Epoch: 7 [0/28539 (0%)]\tLoss: 3.334216\tLR: 0.000571\n",
      "Train Epoch: 7 [1000/28539 (4%)]\tLoss: 2.804429\tLR: 0.000566\n",
      "Train Epoch: 7 [2000/28539 (7%)]\tLoss: 3.137498\tLR: 0.000561\n",
      "Train Epoch: 7 [3000/28539 (11%)]\tLoss: 3.124988\tLR: 0.000556\n",
      "Train Epoch: 7 [4000/28539 (14%)]\tLoss: 3.130549\tLR: 0.000551\n",
      "Train Epoch: 7 [5000/28539 (18%)]\tLoss: 2.979028\tLR: 0.000546\n",
      "Train Epoch: 7 [6000/28539 (21%)]\tLoss: 3.025140\tLR: 0.000541\n",
      "Train Epoch: 7 [7000/28539 (25%)]\tLoss: 2.985048\tLR: 0.000536\n",
      "Train Epoch: 7 [8000/28539 (28%)]\tLoss: 3.280426\tLR: 0.000531\n",
      "Train Epoch: 7 [9000/28539 (32%)]\tLoss: 3.230204\tLR: 0.000526\n",
      "Train Epoch: 7 [10000/28539 (35%)]\tLoss: 2.648834\tLR: 0.000521\n",
      "Train Epoch: 7 [11000/28539 (39%)]\tLoss: 3.102169\tLR: 0.000516\n",
      "Train Epoch: 7 [12000/28539 (42%)]\tLoss: 3.158849\tLR: 0.000511\n",
      "Train Epoch: 7 [13000/28539 (46%)]\tLoss: 2.892703\tLR: 0.000506\n",
      "Train Epoch: 7 [14000/28539 (49%)]\tLoss: 2.976163\tLR: 0.000501\n",
      "Train Epoch: 7 [15000/28539 (53%)]\tLoss: 3.171886\tLR: 0.000496\n",
      "Train Epoch: 7 [16000/28539 (56%)]\tLoss: 2.965230\tLR: 0.000491\n",
      "Train Epoch: 7 [17000/28539 (60%)]\tLoss: 2.838431\tLR: 0.000486\n",
      "Train Epoch: 7 [18000/28539 (63%)]\tLoss: 2.801284\tLR: 0.000481\n",
      "Train Epoch: 7 [19000/28539 (67%)]\tLoss: 3.374784\tLR: 0.000476\n",
      "Train Epoch: 7 [20000/28539 (70%)]\tLoss: 3.007375\tLR: 0.000471\n",
      "Train Epoch: 7 [21000/28539 (74%)]\tLoss: 2.700093\tLR: 0.000466\n",
      "Train Epoch: 7 [22000/28539 (77%)]\tLoss: 3.189711\tLR: 0.000461\n",
      "Train Epoch: 7 [23000/28539 (81%)]\tLoss: 2.662945\tLR: 0.000456\n",
      "Train Epoch: 7 [24000/28539 (84%)]\tLoss: 3.076174\tLR: 0.000451\n",
      "Train Epoch: 7 [25000/28539 (88%)]\tLoss: 2.799485\tLR: 0.000446\n",
      "Train Epoch: 7 [26000/28539 (91%)]\tLoss: 3.119461\tLR: 0.000441\n",
      "Train Epoch: 7 [27000/28539 (95%)]\tLoss: 3.010448\tLR: 0.000436\n",
      "Train Epoch: 7 [28000/28539 (98%)]\tLoss: 2.788466\tLR: 0.000431\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.4299, Average CER: 0.350081 Average WER: 0.5435\n",
      "\n",
      "Fri May  7 05:25:04 UTC 2021\n",
      "Train Epoch: 8 [0/28539 (0%)]\tLoss: 2.826582\tLR: 0.000428\n",
      "Train Epoch: 8 [1000/28539 (4%)]\tLoss: 2.666465\tLR: 0.000423\n",
      "Train Epoch: 8 [2000/28539 (7%)]\tLoss: 3.306512\tLR: 0.000418\n",
      "Train Epoch: 8 [3000/28539 (11%)]\tLoss: 2.650405\tLR: 0.000413\n",
      "Train Epoch: 8 [4000/28539 (14%)]\tLoss: 2.698228\tLR: 0.000408\n",
      "Train Epoch: 8 [5000/28539 (18%)]\tLoss: 2.746639\tLR: 0.000403\n",
      "Train Epoch: 8 [6000/28539 (21%)]\tLoss: 2.668901\tLR: 0.000398\n",
      "Train Epoch: 8 [7000/28539 (25%)]\tLoss: 2.815609\tLR: 0.000393\n",
      "Train Epoch: 8 [8000/28539 (28%)]\tLoss: 2.759000\tLR: 0.000388\n",
      "Train Epoch: 8 [9000/28539 (32%)]\tLoss: 2.627954\tLR: 0.000383\n",
      "Train Epoch: 8 [10000/28539 (35%)]\tLoss: 2.615287\tLR: 0.000378\n",
      "Train Epoch: 8 [11000/28539 (39%)]\tLoss: 2.875061\tLR: 0.000373\n",
      "Train Epoch: 8 [12000/28539 (42%)]\tLoss: 2.301602\tLR: 0.000368\n",
      "Train Epoch: 8 [13000/28539 (46%)]\tLoss: 3.102072\tLR: 0.000363\n",
      "Train Epoch: 8 [14000/28539 (49%)]\tLoss: 2.806409\tLR: 0.000358\n",
      "Train Epoch: 8 [15000/28539 (53%)]\tLoss: 2.754589\tLR: 0.000353\n",
      "Train Epoch: 8 [16000/28539 (56%)]\tLoss: 2.908350\tLR: 0.000348\n",
      "Train Epoch: 8 [17000/28539 (60%)]\tLoss: 2.720193\tLR: 0.000343\n",
      "Train Epoch: 8 [18000/28539 (63%)]\tLoss: 2.778254\tLR: 0.000338\n",
      "Train Epoch: 8 [19000/28539 (67%)]\tLoss: 2.567436\tLR: 0.000333\n",
      "Train Epoch: 8 [20000/28539 (70%)]\tLoss: 2.797033\tLR: 0.000328\n",
      "Train Epoch: 8 [21000/28539 (74%)]\tLoss: 2.460909\tLR: 0.000323\n",
      "Train Epoch: 8 [22000/28539 (77%)]\tLoss: 2.618623\tLR: 0.000318\n",
      "Train Epoch: 8 [23000/28539 (81%)]\tLoss: 2.598593\tLR: 0.000313\n",
      "Train Epoch: 8 [24000/28539 (84%)]\tLoss: 2.663999\tLR: 0.000308\n",
      "Train Epoch: 8 [25000/28539 (88%)]\tLoss: 2.869932\tLR: 0.000303\n",
      "Train Epoch: 8 [26000/28539 (91%)]\tLoss: 2.629640\tLR: 0.000298\n",
      "Train Epoch: 8 [27000/28539 (95%)]\tLoss: 2.788873\tLR: 0.000293\n",
      "Train Epoch: 8 [28000/28539 (98%)]\tLoss: 3.038756\tLR: 0.000288\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.2498, Average CER: 0.321328 Average WER: 0.5146\n",
      "\n",
      "Fri May  7 05:38:35 UTC 2021\n",
      "Train Epoch: 9 [0/28539 (0%)]\tLoss: 2.457594\tLR: 0.000286\n",
      "Train Epoch: 9 [1000/28539 (4%)]\tLoss: 2.776510\tLR: 0.000281\n",
      "Train Epoch: 9 [2000/28539 (7%)]\tLoss: 2.574702\tLR: 0.000276\n",
      "Train Epoch: 9 [3000/28539 (11%)]\tLoss: 2.541619\tLR: 0.000271\n",
      "Train Epoch: 9 [4000/28539 (14%)]\tLoss: 2.534998\tLR: 0.000266\n",
      "Train Epoch: 9 [5000/28539 (18%)]\tLoss: 2.873022\tLR: 0.000261\n",
      "Train Epoch: 9 [6000/28539 (21%)]\tLoss: 2.479013\tLR: 0.000256\n",
      "Train Epoch: 9 [7000/28539 (25%)]\tLoss: 2.614831\tLR: 0.000251\n",
      "Train Epoch: 9 [8000/28539 (28%)]\tLoss: 2.544354\tLR: 0.000246\n",
      "Train Epoch: 9 [9000/28539 (32%)]\tLoss: 2.363555\tLR: 0.000241\n",
      "Train Epoch: 9 [10000/28539 (35%)]\tLoss: 2.418520\tLR: 0.000236\n",
      "Train Epoch: 9 [11000/28539 (39%)]\tLoss: 2.357858\tLR: 0.000231\n",
      "Train Epoch: 9 [12000/28539 (42%)]\tLoss: 2.817200\tLR: 0.000226\n",
      "Train Epoch: 9 [13000/28539 (46%)]\tLoss: 2.821592\tLR: 0.000221\n",
      "Train Epoch: 9 [14000/28539 (49%)]\tLoss: 2.456940\tLR: 0.000216\n",
      "Train Epoch: 9 [15000/28539 (53%)]\tLoss: 2.396533\tLR: 0.000211\n",
      "Train Epoch: 9 [16000/28539 (56%)]\tLoss: 2.755362\tLR: 0.000206\n",
      "Train Epoch: 9 [17000/28539 (60%)]\tLoss: 2.998589\tLR: 0.000201\n",
      "Train Epoch: 9 [18000/28539 (63%)]\tLoss: 2.574760\tLR: 0.000196\n",
      "Train Epoch: 9 [19000/28539 (67%)]\tLoss: 2.718533\tLR: 0.000191\n",
      "Train Epoch: 9 [20000/28539 (70%)]\tLoss: 2.500628\tLR: 0.000186\n",
      "Train Epoch: 9 [21000/28539 (74%)]\tLoss: 2.559662\tLR: 0.000181\n",
      "Train Epoch: 9 [22000/28539 (77%)]\tLoss: 2.514986\tLR: 0.000175\n",
      "Train Epoch: 9 [23000/28539 (81%)]\tLoss: 2.257437\tLR: 0.000170\n",
      "Train Epoch: 9 [24000/28539 (84%)]\tLoss: 2.808647\tLR: 0.000165\n",
      "Train Epoch: 9 [25000/28539 (88%)]\tLoss: 2.623723\tLR: 0.000160\n",
      "Train Epoch: 9 [26000/28539 (91%)]\tLoss: 2.651878\tLR: 0.000155\n",
      "Train Epoch: 9 [27000/28539 (95%)]\tLoss: 2.926601\tLR: 0.000150\n",
      "Train Epoch: 9 [28000/28539 (98%)]\tLoss: 2.432024\tLR: 0.000145\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.1174, Average CER: 0.307951 Average WER: 0.4916\n",
      "\n",
      "Fri May  7 05:52:04 UTC 2021\n",
      "Train Epoch: 10 [0/28539 (0%)]\tLoss: 2.342887\tLR: 0.000143\n",
      "Train Epoch: 10 [1000/28539 (4%)]\tLoss: 2.468745\tLR: 0.000138\n",
      "Train Epoch: 10 [2000/28539 (7%)]\tLoss: 2.527611\tLR: 0.000133\n",
      "Train Epoch: 10 [3000/28539 (11%)]\tLoss: 2.290834\tLR: 0.000128\n",
      "Train Epoch: 10 [4000/28539 (14%)]\tLoss: 1.916484\tLR: 0.000123\n",
      "Train Epoch: 10 [5000/28539 (18%)]\tLoss: 2.338745\tLR: 0.000118\n",
      "Train Epoch: 10 [6000/28539 (21%)]\tLoss: 2.358880\tLR: 0.000113\n",
      "Train Epoch: 10 [7000/28539 (25%)]\tLoss: 2.311330\tLR: 0.000108\n",
      "Train Epoch: 10 [8000/28539 (28%)]\tLoss: 2.253510\tLR: 0.000103\n",
      "Train Epoch: 10 [9000/28539 (32%)]\tLoss: 2.483710\tLR: 0.000098\n",
      "Train Epoch: 10 [10000/28539 (35%)]\tLoss: 2.712044\tLR: 0.000093\n",
      "Train Epoch: 10 [11000/28539 (39%)]\tLoss: 2.334636\tLR: 0.000088\n",
      "Train Epoch: 10 [12000/28539 (42%)]\tLoss: 2.196744\tLR: 0.000083\n",
      "Train Epoch: 10 [13000/28539 (46%)]\tLoss: 2.413106\tLR: 0.000078\n",
      "Train Epoch: 10 [14000/28539 (49%)]\tLoss: 2.380363\tLR: 0.000073\n",
      "Train Epoch: 10 [15000/28539 (53%)]\tLoss: 2.474442\tLR: 0.000068\n",
      "Train Epoch: 10 [16000/28539 (56%)]\tLoss: 2.603714\tLR: 0.000063\n",
      "Train Epoch: 10 [17000/28539 (60%)]\tLoss: 2.421567\tLR: 0.000058\n",
      "Train Epoch: 10 [18000/28539 (63%)]\tLoss: 2.428222\tLR: 0.000053\n",
      "Train Epoch: 10 [19000/28539 (67%)]\tLoss: 2.562131\tLR: 0.000048\n",
      "Train Epoch: 10 [20000/28539 (70%)]\tLoss: 2.378673\tLR: 0.000043\n",
      "Train Epoch: 10 [21000/28539 (74%)]\tLoss: 2.265824\tLR: 0.000038\n",
      "Train Epoch: 10 [22000/28539 (77%)]\tLoss: 2.495062\tLR: 0.000033\n",
      "Train Epoch: 10 [23000/28539 (81%)]\tLoss: 2.482983\tLR: 0.000028\n",
      "Train Epoch: 10 [24000/28539 (84%)]\tLoss: 2.432346\tLR: 0.000023\n",
      "Train Epoch: 10 [25000/28539 (88%)]\tLoss: 2.701950\tLR: 0.000018\n",
      "Train Epoch: 10 [26000/28539 (91%)]\tLoss: 2.410025\tLR: 0.000013\n",
      "Train Epoch: 10 [27000/28539 (95%)]\tLoss: 1.874501\tLR: 0.000008\n",
      "Train Epoch: 10 [28000/28539 (98%)]\tLoss: 2.076552\tLR: 0.000003\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.0516, Average CER: 0.293512 Average WER: 0.4801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 10\n",
    "test_batch_size = 7\n",
    "epochs = 10\n",
    "libri_train_set = \"train-clean-100\"\n",
    "libri_test_set = \"test-clean\"\n",
    "\n",
    "main(learning_rate, batch_size, test_batch_size, epochs, libri_train_set, libri_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J14QQbRDb6vB"
   },
   "source": [
    "Качество распознавания немного улучшилось, но, кажется, модель нужно обучать дольше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV48Q7HqZsAD"
   },
   "source": [
    "### <b>Задание №2</b> (5 баллов):\n",
    "Импровизация по улучшению качества распознавания.\n",
    "\n",
    "Пробовал уменьшать модель - качество сильно просело без существенного выигрыша в скорости.\n",
    "\n",
    "2 финальных запуска - при увеличенном до 20-ти кол-ве слоев и смещенном на 0.7 шедулере при LR = 3e-3. результаты улучшить не получилось ((("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 992,
     "status": "ok",
     "timestamp": 1620391486514,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "OESWsXPscUuZ"
   },
   "outputs": [],
   "source": [
    "NUM_UNITS = 4000\n",
    "BLANK = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 2818,
     "status": "ok",
     "timestamp": 1620391488794,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "EQvXYlwDc7y_"
   },
   "outputs": [],
   "source": [
    "text_transform = TextTransformBPE('train_clean_100_text_clean.txt', num_units=NUM_UNITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 1838,
     "status": "ok",
     "timestamp": 1620372220579,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "j24csXVrcd7b"
   },
   "outputs": [],
   "source": [
    "#Num layers changed & attention params\n",
    "def main(learning_rate=1e-5, batch_size=20, test_batch_size=7, epochs=10,\n",
    "        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
    "    \n",
    "    hparams = {\n",
    "        \"input_size\": 80,\n",
    "        \"output_size\": NUM_UNITS + 1,\n",
    "        \"conv2d_filters\": 32,\n",
    "        \"attention_dim\": 360,\n",
    "        \"attention_heads\": 8,\n",
    "        \"feedforward_dim\": 1024,\n",
    "        \"num_layers\":20,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=test_batch_size,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "    model = TransformerModel(\n",
    "        hparams['input_size'],\n",
    "        hparams['output_size'],\n",
    "        hparams['conv2d_filters'],\n",
    "        hparams['attention_dim'],\n",
    "        hparams['attention_heads'],\n",
    "        hparams['feedforward_dim'],\n",
    "        hparams['num_layers'],\n",
    "        hparams['dropout']).to(device)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = torch.nn.CTCLoss(blank=BLANK, zero_infinity=False).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        !date\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "        test(model, device, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4791003,
     "status": "ok",
     "timestamp": 1620384005416,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "pbmlzUXecrwc",
    "outputId": "57655d5d-3edd-42f4-e413-7f26465d2d8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (conv_in): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_out): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=360, bias=True)\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer): MultiSequential(\n",
      "    (0): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (1): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (2): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (3): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (4): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (5): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (6): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (7): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (8): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (9): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (10): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (11): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (12): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (13): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (14): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (15): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (16): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (17): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (18): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (19): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (after_norm): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "  (final_layer): Linear(in_features=360, out_features=4001, bias=True)\n",
      ")\n",
      "Num Model Parameters 26884289\n",
      "Fri May  7 07:23:48 UTC 2021\n",
      "Train Epoch: 1 [0/28539 (0%)]\tLoss: 54.431286\tLR: 0.000040\n",
      "Train Epoch: 1 [1000/28539 (4%)]\tLoss: 7.672234\tLR: 0.000051\n",
      "Train Epoch: 1 [2000/28539 (7%)]\tLoss: 6.971549\tLR: 0.000063\n",
      "Train Epoch: 1 [3000/28539 (11%)]\tLoss: 7.067052\tLR: 0.000074\n",
      "Train Epoch: 1 [4000/28539 (14%)]\tLoss: 6.854688\tLR: 0.000085\n",
      "Train Epoch: 1 [5000/28539 (18%)]\tLoss: 6.677019\tLR: 0.000096\n",
      "Train Epoch: 1 [6000/28539 (21%)]\tLoss: 6.915543\tLR: 0.000107\n",
      "Train Epoch: 1 [7000/28539 (25%)]\tLoss: 6.759080\tLR: 0.000119\n",
      "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 6.996573\tLR: 0.000130\n",
      "Train Epoch: 1 [9000/28539 (32%)]\tLoss: 6.820657\tLR: 0.000141\n",
      "Train Epoch: 1 [10000/28539 (35%)]\tLoss: 7.009841\tLR: 0.000152\n",
      "Train Epoch: 1 [11000/28539 (39%)]\tLoss: 7.000979\tLR: 0.000163\n",
      "Train Epoch: 1 [12000/28539 (42%)]\tLoss: 6.827300\tLR: 0.000175\n",
      "Train Epoch: 1 [13000/28539 (46%)]\tLoss: 6.836089\tLR: 0.000186\n",
      "Train Epoch: 1 [14000/28539 (49%)]\tLoss: 6.827811\tLR: 0.000197\n",
      "Train Epoch: 1 [15000/28539 (53%)]\tLoss: 7.121326\tLR: 0.000208\n",
      "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 6.606972\tLR: 0.000220\n",
      "Train Epoch: 1 [17000/28539 (60%)]\tLoss: 6.937024\tLR: 0.000231\n",
      "Train Epoch: 1 [18000/28539 (63%)]\tLoss: 6.856567\tLR: 0.000242\n",
      "Train Epoch: 1 [19000/28539 (67%)]\tLoss: 6.876822\tLR: 0.000253\n",
      "Train Epoch: 1 [20000/28539 (70%)]\tLoss: 7.167412\tLR: 0.000264\n",
      "Train Epoch: 1 [21000/28539 (74%)]\tLoss: 6.658133\tLR: 0.000276\n",
      "Train Epoch: 1 [22000/28539 (77%)]\tLoss: 6.812833\tLR: 0.000287\n",
      "Train Epoch: 1 [23000/28539 (81%)]\tLoss: 6.941288\tLR: 0.000298\n",
      "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 6.609090\tLR: 0.000309\n",
      "Train Epoch: 1 [25000/28539 (88%)]\tLoss: 6.700212\tLR: 0.000320\n",
      "Train Epoch: 1 [26000/28539 (91%)]\tLoss: 6.770359\tLR: 0.000332\n",
      "Train Epoch: 1 [27000/28539 (95%)]\tLoss: 6.605329\tLR: 0.000343\n",
      "Train Epoch: 1 [28000/28539 (98%)]\tLoss: 6.706581\tLR: 0.000354\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 6.7334, Average CER: 0.999347 Average WER: 0.9992\n",
      "\n",
      "Fri May  7 07:40:00 UTC 2021\n",
      "Train Epoch: 2 [0/28539 (0%)]\tLoss: 6.729179\tLR: 0.000360\n",
      "Train Epoch: 2 [1000/28539 (4%)]\tLoss: 6.618312\tLR: 0.000371\n",
      "Train Epoch: 2 [2000/28539 (7%)]\tLoss: 6.558856\tLR: 0.000383\n",
      "Train Epoch: 2 [3000/28539 (11%)]\tLoss: 6.539857\tLR: 0.000394\n",
      "Train Epoch: 2 [4000/28539 (14%)]\tLoss: 6.768625\tLR: 0.000405\n",
      "Train Epoch: 2 [5000/28539 (18%)]\tLoss: 6.464036\tLR: 0.000416\n",
      "Train Epoch: 2 [6000/28539 (21%)]\tLoss: 6.482896\tLR: 0.000427\n",
      "Train Epoch: 2 [7000/28539 (25%)]\tLoss: 6.388930\tLR: 0.000439\n",
      "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 6.082590\tLR: 0.000450\n",
      "Train Epoch: 2 [9000/28539 (32%)]\tLoss: 6.383111\tLR: 0.000461\n",
      "Train Epoch: 2 [10000/28539 (35%)]\tLoss: 6.130055\tLR: 0.000472\n",
      "Train Epoch: 2 [11000/28539 (39%)]\tLoss: 6.107994\tLR: 0.000483\n",
      "Train Epoch: 2 [12000/28539 (42%)]\tLoss: 6.106939\tLR: 0.000495\n",
      "Train Epoch: 2 [13000/28539 (46%)]\tLoss: 6.084261\tLR: 0.000506\n",
      "Train Epoch: 2 [14000/28539 (49%)]\tLoss: 5.974831\tLR: 0.000517\n",
      "Train Epoch: 2 [15000/28539 (53%)]\tLoss: 5.947553\tLR: 0.000528\n",
      "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 6.178867\tLR: 0.000540\n",
      "Train Epoch: 2 [17000/28539 (60%)]\tLoss: 5.699866\tLR: 0.000551\n",
      "Train Epoch: 2 [18000/28539 (63%)]\tLoss: 6.069633\tLR: 0.000562\n",
      "Train Epoch: 2 [19000/28539 (67%)]\tLoss: 5.639351\tLR: 0.000573\n",
      "Train Epoch: 2 [20000/28539 (70%)]\tLoss: 5.477966\tLR: 0.000584\n",
      "Train Epoch: 2 [21000/28539 (74%)]\tLoss: 5.406518\tLR: 0.000596\n",
      "Train Epoch: 2 [22000/28539 (77%)]\tLoss: 5.726919\tLR: 0.000607\n",
      "Train Epoch: 2 [23000/28539 (81%)]\tLoss: 5.701778\tLR: 0.000618\n",
      "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 5.491403\tLR: 0.000629\n",
      "Train Epoch: 2 [25000/28539 (88%)]\tLoss: 5.097640\tLR: 0.000640\n",
      "Train Epoch: 2 [26000/28539 (91%)]\tLoss: 5.358449\tLR: 0.000652\n",
      "Train Epoch: 2 [27000/28539 (95%)]\tLoss: 5.279584\tLR: 0.000663\n",
      "Train Epoch: 2 [28000/28539 (98%)]\tLoss: 5.479563\tLR: 0.000674\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 5.0313, Average CER: 0.790770 Average WER: 0.8775\n",
      "\n",
      "Fri May  7 07:57:38 UTC 2021\n",
      "Train Epoch: 3 [0/28539 (0%)]\tLoss: 5.195200\tLR: 0.000680\n",
      "Train Epoch: 3 [1000/28539 (4%)]\tLoss: 5.015248\tLR: 0.000691\n",
      "Train Epoch: 3 [2000/28539 (7%)]\tLoss: 5.034328\tLR: 0.000703\n",
      "Train Epoch: 3 [3000/28539 (11%)]\tLoss: 5.030243\tLR: 0.000714\n",
      "Train Epoch: 3 [4000/28539 (14%)]\tLoss: 4.915899\tLR: 0.000725\n",
      "Train Epoch: 3 [5000/28539 (18%)]\tLoss: 5.302010\tLR: 0.000736\n",
      "Train Epoch: 3 [6000/28539 (21%)]\tLoss: 5.012348\tLR: 0.000747\n",
      "Train Epoch: 3 [7000/28539 (25%)]\tLoss: 5.220179\tLR: 0.000759\n",
      "Train Epoch: 3 [8000/28539 (28%)]\tLoss: 5.091670\tLR: 0.000770\n",
      "Train Epoch: 3 [9000/28539 (32%)]\tLoss: 5.443466\tLR: 0.000781\n",
      "Train Epoch: 3 [10000/28539 (35%)]\tLoss: 5.475566\tLR: 0.000792\n",
      "Train Epoch: 3 [11000/28539 (39%)]\tLoss: 4.834494\tLR: 0.000804\n",
      "Train Epoch: 3 [12000/28539 (42%)]\tLoss: 5.036412\tLR: 0.000815\n",
      "Train Epoch: 3 [13000/28539 (46%)]\tLoss: 5.017938\tLR: 0.000826\n",
      "Train Epoch: 3 [14000/28539 (49%)]\tLoss: 5.158020\tLR: 0.000837\n",
      "Train Epoch: 3 [15000/28539 (53%)]\tLoss: 4.946417\tLR: 0.000848\n",
      "Train Epoch: 3 [16000/28539 (56%)]\tLoss: 5.030555\tLR: 0.000860\n",
      "Train Epoch: 3 [17000/28539 (60%)]\tLoss: 4.952146\tLR: 0.000871\n",
      "Train Epoch: 3 [18000/28539 (63%)]\tLoss: 5.035259\tLR: 0.000882\n",
      "Train Epoch: 3 [19000/28539 (67%)]\tLoss: 5.082526\tLR: 0.000893\n",
      "Train Epoch: 3 [20000/28539 (70%)]\tLoss: 4.955812\tLR: 0.000904\n",
      "Train Epoch: 3 [21000/28539 (74%)]\tLoss: 4.866120\tLR: 0.000916\n",
      "Train Epoch: 3 [22000/28539 (77%)]\tLoss: 4.913369\tLR: 0.000927\n",
      "Train Epoch: 3 [23000/28539 (81%)]\tLoss: 4.936677\tLR: 0.000938\n",
      "Train Epoch: 3 [24000/28539 (84%)]\tLoss: 4.723676\tLR: 0.000949\n",
      "Train Epoch: 3 [25000/28539 (88%)]\tLoss: 5.146142\tLR: 0.000961\n",
      "Train Epoch: 3 [26000/28539 (91%)]\tLoss: 4.896994\tLR: 0.000972\n",
      "Train Epoch: 3 [27000/28539 (95%)]\tLoss: 4.982101\tLR: 0.000983\n",
      "Train Epoch: 3 [28000/28539 (98%)]\tLoss: 4.771398\tLR: 0.000994\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 4.4226, Average CER: 0.684066 Average WER: 0.8191\n",
      "\n",
      "Fri May  7 08:16:17 UTC 2021\n",
      "Train Epoch: 4 [0/28539 (0%)]\tLoss: 4.690057\tLR: 0.001000\n",
      "Train Epoch: 4 [1000/28539 (4%)]\tLoss: 4.972970\tLR: 0.000995\n",
      "Train Epoch: 4 [2000/28539 (7%)]\tLoss: 4.609060\tLR: 0.000990\n",
      "Train Epoch: 4 [3000/28539 (11%)]\tLoss: 4.717878\tLR: 0.000985\n",
      "Train Epoch: 4 [4000/28539 (14%)]\tLoss: 4.880617\tLR: 0.000980\n",
      "Train Epoch: 4 [5000/28539 (18%)]\tLoss: 4.558040\tLR: 0.000975\n",
      "Train Epoch: 4 [6000/28539 (21%)]\tLoss: 4.619483\tLR: 0.000970\n",
      "Train Epoch: 4 [7000/28539 (25%)]\tLoss: 4.525322\tLR: 0.000965\n",
      "Train Epoch: 4 [8000/28539 (28%)]\tLoss: 4.637105\tLR: 0.000960\n",
      "Train Epoch: 4 [9000/28539 (32%)]\tLoss: 4.258828\tLR: 0.000955\n",
      "Train Epoch: 4 [10000/28539 (35%)]\tLoss: 4.697290\tLR: 0.000950\n",
      "Train Epoch: 4 [11000/28539 (39%)]\tLoss: 4.516418\tLR: 0.000945\n",
      "Train Epoch: 4 [12000/28539 (42%)]\tLoss: 4.698847\tLR: 0.000940\n",
      "Train Epoch: 4 [13000/28539 (46%)]\tLoss: 4.607145\tLR: 0.000935\n",
      "Train Epoch: 4 [14000/28539 (49%)]\tLoss: 5.113527\tLR: 0.000930\n",
      "Train Epoch: 4 [15000/28539 (53%)]\tLoss: 4.236095\tLR: 0.000925\n",
      "Train Epoch: 4 [16000/28539 (56%)]\tLoss: 4.212198\tLR: 0.000920\n",
      "Train Epoch: 4 [17000/28539 (60%)]\tLoss: 4.743978\tLR: 0.000915\n",
      "Train Epoch: 4 [18000/28539 (63%)]\tLoss: 4.330761\tLR: 0.000910\n",
      "Train Epoch: 4 [19000/28539 (67%)]\tLoss: 4.791727\tLR: 0.000905\n",
      "Train Epoch: 4 [20000/28539 (70%)]\tLoss: 4.658875\tLR: 0.000900\n",
      "Train Epoch: 4 [21000/28539 (74%)]\tLoss: 4.694829\tLR: 0.000895\n",
      "Train Epoch: 4 [22000/28539 (77%)]\tLoss: 4.306149\tLR: 0.000890\n",
      "Train Epoch: 4 [23000/28539 (81%)]\tLoss: 4.193806\tLR: 0.000885\n",
      "Train Epoch: 4 [24000/28539 (84%)]\tLoss: 4.342004\tLR: 0.000880\n",
      "Train Epoch: 4 [25000/28539 (88%)]\tLoss: 4.167750\tLR: 0.000875\n",
      "Train Epoch: 4 [26000/28539 (91%)]\tLoss: 4.158037\tLR: 0.000870\n",
      "Train Epoch: 4 [27000/28539 (95%)]\tLoss: 3.986676\tLR: 0.000865\n",
      "Train Epoch: 4 [28000/28539 (98%)]\tLoss: 4.240096\tLR: 0.000860\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.8419, Average CER: 0.566673 Average WER: 0.7398\n",
      "\n",
      "Fri May  7 08:35:53 UTC 2021\n",
      "Train Epoch: 5 [0/28539 (0%)]\tLoss: 4.166894\tLR: 0.000857\n",
      "Train Epoch: 5 [1000/28539 (4%)]\tLoss: 4.543446\tLR: 0.000852\n",
      "Train Epoch: 5 [2000/28539 (7%)]\tLoss: 4.270802\tLR: 0.000847\n",
      "Train Epoch: 5 [3000/28539 (11%)]\tLoss: 4.253836\tLR: 0.000842\n",
      "Train Epoch: 5 [4000/28539 (14%)]\tLoss: 4.262272\tLR: 0.000837\n",
      "Train Epoch: 5 [5000/28539 (18%)]\tLoss: 4.327146\tLR: 0.000832\n",
      "Train Epoch: 5 [6000/28539 (21%)]\tLoss: 4.276567\tLR: 0.000827\n",
      "Train Epoch: 5 [7000/28539 (25%)]\tLoss: 3.812923\tLR: 0.000822\n",
      "Train Epoch: 5 [8000/28539 (28%)]\tLoss: 3.911981\tLR: 0.000817\n",
      "Train Epoch: 5 [9000/28539 (32%)]\tLoss: 4.229952\tLR: 0.000812\n",
      "Train Epoch: 5 [10000/28539 (35%)]\tLoss: 3.713229\tLR: 0.000807\n",
      "Train Epoch: 5 [11000/28539 (39%)]\tLoss: 3.721561\tLR: 0.000802\n",
      "Train Epoch: 5 [12000/28539 (42%)]\tLoss: 4.158195\tLR: 0.000797\n",
      "Train Epoch: 5 [13000/28539 (46%)]\tLoss: 3.708179\tLR: 0.000792\n",
      "Train Epoch: 5 [14000/28539 (49%)]\tLoss: 3.784770\tLR: 0.000787\n",
      "Train Epoch: 5 [15000/28539 (53%)]\tLoss: 4.411576\tLR: 0.000782\n",
      "Train Epoch: 5 [16000/28539 (56%)]\tLoss: 3.610317\tLR: 0.000777\n",
      "Train Epoch: 5 [17000/28539 (60%)]\tLoss: 3.995236\tLR: 0.000772\n",
      "Train Epoch: 5 [18000/28539 (63%)]\tLoss: 3.798016\tLR: 0.000767\n",
      "Train Epoch: 5 [19000/28539 (67%)]\tLoss: 3.876032\tLR: 0.000762\n",
      "Train Epoch: 5 [20000/28539 (70%)]\tLoss: 3.696066\tLR: 0.000757\n",
      "Train Epoch: 5 [21000/28539 (74%)]\tLoss: 3.700498\tLR: 0.000752\n",
      "Train Epoch: 5 [22000/28539 (77%)]\tLoss: 4.089329\tLR: 0.000747\n",
      "Train Epoch: 5 [23000/28539 (81%)]\tLoss: 3.719492\tLR: 0.000742\n",
      "Train Epoch: 5 [24000/28539 (84%)]\tLoss: 3.764156\tLR: 0.000737\n",
      "Train Epoch: 5 [25000/28539 (88%)]\tLoss: 3.758161\tLR: 0.000732\n",
      "Train Epoch: 5 [26000/28539 (91%)]\tLoss: 3.670979\tLR: 0.000727\n",
      "Train Epoch: 5 [27000/28539 (95%)]\tLoss: 3.750962\tLR: 0.000722\n",
      "Train Epoch: 5 [28000/28539 (98%)]\tLoss: 3.789188\tLR: 0.000717\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.2234, Average CER: 0.465284 Average WER: 0.6606\n",
      "\n",
      "Fri May  7 08:56:13 UTC 2021\n",
      "Train Epoch: 6 [0/28539 (0%)]\tLoss: 3.533139\tLR: 0.000714\n",
      "Train Epoch: 6 [1000/28539 (4%)]\tLoss: 3.521428\tLR: 0.000709\n",
      "Train Epoch: 6 [2000/28539 (7%)]\tLoss: 3.704202\tLR: 0.000704\n",
      "Train Epoch: 6 [3000/28539 (11%)]\tLoss: 3.360586\tLR: 0.000699\n",
      "Train Epoch: 6 [4000/28539 (14%)]\tLoss: 3.360049\tLR: 0.000694\n",
      "Train Epoch: 6 [5000/28539 (18%)]\tLoss: 3.493044\tLR: 0.000689\n",
      "Train Epoch: 6 [6000/28539 (21%)]\tLoss: 3.639747\tLR: 0.000684\n",
      "Train Epoch: 6 [7000/28539 (25%)]\tLoss: 3.400958\tLR: 0.000679\n",
      "Train Epoch: 6 [8000/28539 (28%)]\tLoss: 3.800032\tLR: 0.000674\n",
      "Train Epoch: 6 [9000/28539 (32%)]\tLoss: 3.247044\tLR: 0.000669\n",
      "Train Epoch: 6 [10000/28539 (35%)]\tLoss: 3.410547\tLR: 0.000664\n",
      "Train Epoch: 6 [11000/28539 (39%)]\tLoss: 3.669927\tLR: 0.000659\n",
      "Train Epoch: 6 [12000/28539 (42%)]\tLoss: 3.688594\tLR: 0.000654\n",
      "Train Epoch: 6 [13000/28539 (46%)]\tLoss: 3.534463\tLR: 0.000649\n",
      "Train Epoch: 6 [14000/28539 (49%)]\tLoss: 3.277792\tLR: 0.000644\n",
      "Train Epoch: 6 [15000/28539 (53%)]\tLoss: 3.322588\tLR: 0.000639\n",
      "Train Epoch: 6 [16000/28539 (56%)]\tLoss: 3.377644\tLR: 0.000634\n",
      "Train Epoch: 6 [17000/28539 (60%)]\tLoss: 3.188541\tLR: 0.000629\n",
      "Train Epoch: 6 [18000/28539 (63%)]\tLoss: 3.076343\tLR: 0.000624\n",
      "Train Epoch: 6 [19000/28539 (67%)]\tLoss: 3.799398\tLR: 0.000619\n",
      "Train Epoch: 6 [20000/28539 (70%)]\tLoss: 3.439641\tLR: 0.000614\n",
      "Train Epoch: 6 [21000/28539 (74%)]\tLoss: 3.281632\tLR: 0.000609\n",
      "Train Epoch: 6 [22000/28539 (77%)]\tLoss: 3.616604\tLR: 0.000604\n",
      "Train Epoch: 6 [23000/28539 (81%)]\tLoss: 3.523512\tLR: 0.000599\n",
      "Train Epoch: 6 [24000/28539 (84%)]\tLoss: 3.388197\tLR: 0.000594\n",
      "Train Epoch: 6 [25000/28539 (88%)]\tLoss: 3.546642\tLR: 0.000589\n",
      "Train Epoch: 6 [26000/28539 (91%)]\tLoss: 3.859590\tLR: 0.000584\n",
      "Train Epoch: 6 [27000/28539 (95%)]\tLoss: 3.451641\tLR: 0.000579\n",
      "Train Epoch: 6 [28000/28539 (98%)]\tLoss: 3.002116\tLR: 0.000574\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.8149, Average CER: 0.412072 Average WER: 0.6076\n",
      "\n",
      "Fri May  7 09:16:40 UTC 2021\n",
      "Train Epoch: 7 [0/28539 (0%)]\tLoss: 3.037205\tLR: 0.000571\n",
      "Train Epoch: 7 [1000/28539 (4%)]\tLoss: 3.418952\tLR: 0.000566\n",
      "Train Epoch: 7 [2000/28539 (7%)]\tLoss: 3.340464\tLR: 0.000561\n",
      "Train Epoch: 7 [3000/28539 (11%)]\tLoss: 3.110263\tLR: 0.000556\n",
      "Train Epoch: 7 [4000/28539 (14%)]\tLoss: 3.184797\tLR: 0.000551\n",
      "Train Epoch: 7 [5000/28539 (18%)]\tLoss: 2.830672\tLR: 0.000546\n",
      "Train Epoch: 7 [6000/28539 (21%)]\tLoss: 3.284266\tLR: 0.000541\n",
      "Train Epoch: 7 [7000/28539 (25%)]\tLoss: 3.060947\tLR: 0.000536\n",
      "Train Epoch: 7 [8000/28539 (28%)]\tLoss: 2.993276\tLR: 0.000531\n",
      "Train Epoch: 7 [9000/28539 (32%)]\tLoss: 3.139126\tLR: 0.000526\n",
      "Train Epoch: 7 [10000/28539 (35%)]\tLoss: 3.226067\tLR: 0.000521\n",
      "Train Epoch: 7 [11000/28539 (39%)]\tLoss: 3.062347\tLR: 0.000516\n",
      "Train Epoch: 7 [12000/28539 (42%)]\tLoss: 3.424763\tLR: 0.000511\n",
      "Train Epoch: 7 [13000/28539 (46%)]\tLoss: 3.052599\tLR: 0.000506\n",
      "Train Epoch: 7 [14000/28539 (49%)]\tLoss: 2.784822\tLR: 0.000501\n",
      "Train Epoch: 7 [15000/28539 (53%)]\tLoss: 3.026661\tLR: 0.000496\n",
      "Train Epoch: 7 [16000/28539 (56%)]\tLoss: 3.476246\tLR: 0.000491\n",
      "Train Epoch: 7 [17000/28539 (60%)]\tLoss: 3.086970\tLR: 0.000486\n",
      "Train Epoch: 7 [18000/28539 (63%)]\tLoss: 3.427088\tLR: 0.000481\n",
      "Train Epoch: 7 [19000/28539 (67%)]\tLoss: 2.839562\tLR: 0.000476\n",
      "Train Epoch: 7 [20000/28539 (70%)]\tLoss: 2.817531\tLR: 0.000471\n",
      "Train Epoch: 7 [21000/28539 (74%)]\tLoss: 3.062001\tLR: 0.000466\n",
      "Train Epoch: 7 [22000/28539 (77%)]\tLoss: 2.929519\tLR: 0.000461\n",
      "Train Epoch: 7 [23000/28539 (81%)]\tLoss: 3.105434\tLR: 0.000456\n",
      "Train Epoch: 7 [24000/28539 (84%)]\tLoss: 3.369322\tLR: 0.000451\n",
      "Train Epoch: 7 [25000/28539 (88%)]\tLoss: 3.201687\tLR: 0.000446\n",
      "Train Epoch: 7 [26000/28539 (91%)]\tLoss: 2.909541\tLR: 0.000441\n",
      "Train Epoch: 7 [27000/28539 (95%)]\tLoss: 2.837054\tLR: 0.000436\n",
      "Train Epoch: 7 [28000/28539 (98%)]\tLoss: 2.929294\tLR: 0.000431\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.5297, Average CER: 0.362711 Average WER: 0.5609\n",
      "\n",
      "Fri May  7 09:37:24 UTC 2021\n",
      "Train Epoch: 8 [0/28539 (0%)]\tLoss: 2.799398\tLR: 0.000428\n",
      "Train Epoch: 8 [1000/28539 (4%)]\tLoss: 2.780242\tLR: 0.000423\n",
      "Train Epoch: 8 [2000/28539 (7%)]\tLoss: 2.941854\tLR: 0.000418\n",
      "Train Epoch: 8 [3000/28539 (11%)]\tLoss: 2.807321\tLR: 0.000413\n",
      "Train Epoch: 8 [4000/28539 (14%)]\tLoss: 2.699171\tLR: 0.000408\n",
      "Train Epoch: 8 [5000/28539 (18%)]\tLoss: 3.101250\tLR: 0.000403\n",
      "Train Epoch: 8 [6000/28539 (21%)]\tLoss: 2.385642\tLR: 0.000398\n",
      "Train Epoch: 8 [7000/28539 (25%)]\tLoss: 3.107652\tLR: 0.000393\n",
      "Train Epoch: 8 [8000/28539 (28%)]\tLoss: 2.839745\tLR: 0.000388\n",
      "Train Epoch: 8 [9000/28539 (32%)]\tLoss: 2.734118\tLR: 0.000383\n",
      "Train Epoch: 8 [10000/28539 (35%)]\tLoss: 2.340160\tLR: 0.000378\n",
      "Train Epoch: 8 [11000/28539 (39%)]\tLoss: 2.850207\tLR: 0.000373\n",
      "Train Epoch: 8 [12000/28539 (42%)]\tLoss: 3.256748\tLR: 0.000368\n",
      "Train Epoch: 8 [13000/28539 (46%)]\tLoss: 3.024569\tLR: 0.000363\n",
      "Train Epoch: 8 [14000/28539 (49%)]\tLoss: 2.567833\tLR: 0.000358\n",
      "Train Epoch: 8 [15000/28539 (53%)]\tLoss: 2.869496\tLR: 0.000353\n",
      "Train Epoch: 8 [16000/28539 (56%)]\tLoss: 2.617552\tLR: 0.000348\n",
      "Train Epoch: 8 [17000/28539 (60%)]\tLoss: 2.868677\tLR: 0.000343\n",
      "Train Epoch: 8 [18000/28539 (63%)]\tLoss: 2.776272\tLR: 0.000338\n",
      "Train Epoch: 8 [19000/28539 (67%)]\tLoss: 2.761031\tLR: 0.000333\n",
      "Train Epoch: 8 [20000/28539 (70%)]\tLoss: 2.961111\tLR: 0.000328\n",
      "Train Epoch: 8 [21000/28539 (74%)]\tLoss: 3.210059\tLR: 0.000323\n",
      "Train Epoch: 8 [22000/28539 (77%)]\tLoss: 2.391919\tLR: 0.000318\n",
      "Train Epoch: 8 [23000/28539 (81%)]\tLoss: 2.806069\tLR: 0.000313\n",
      "Train Epoch: 8 [24000/28539 (84%)]\tLoss: 3.184729\tLR: 0.000308\n",
      "Train Epoch: 8 [25000/28539 (88%)]\tLoss: 3.144836\tLR: 0.000303\n",
      "Train Epoch: 8 [26000/28539 (91%)]\tLoss: 2.849475\tLR: 0.000298\n",
      "Train Epoch: 8 [27000/28539 (95%)]\tLoss: 2.387450\tLR: 0.000293\n",
      "Train Epoch: 8 [28000/28539 (98%)]\tLoss: 2.452918\tLR: 0.000288\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.3149, Average CER: 0.329140 Average WER: 0.5275\n",
      "\n",
      "Fri May  7 09:58:15 UTC 2021\n",
      "Train Epoch: 9 [0/28539 (0%)]\tLoss: 2.346323\tLR: 0.000286\n",
      "Train Epoch: 9 [1000/28539 (4%)]\tLoss: 2.411779\tLR: 0.000281\n",
      "Train Epoch: 9 [2000/28539 (7%)]\tLoss: 2.757460\tLR: 0.000276\n",
      "Train Epoch: 9 [3000/28539 (11%)]\tLoss: 2.577984\tLR: 0.000271\n",
      "Train Epoch: 9 [4000/28539 (14%)]\tLoss: 2.791819\tLR: 0.000266\n",
      "Train Epoch: 9 [5000/28539 (18%)]\tLoss: 2.759070\tLR: 0.000261\n",
      "Train Epoch: 9 [6000/28539 (21%)]\tLoss: 2.356783\tLR: 0.000256\n",
      "Train Epoch: 9 [7000/28539 (25%)]\tLoss: 2.707265\tLR: 0.000251\n",
      "Train Epoch: 9 [8000/28539 (28%)]\tLoss: 2.918669\tLR: 0.000246\n",
      "Train Epoch: 9 [9000/28539 (32%)]\tLoss: 2.413807\tLR: 0.000241\n",
      "Train Epoch: 9 [10000/28539 (35%)]\tLoss: 2.665319\tLR: 0.000236\n",
      "Train Epoch: 9 [11000/28539 (39%)]\tLoss: 2.412975\tLR: 0.000231\n",
      "Train Epoch: 9 [12000/28539 (42%)]\tLoss: 2.475791\tLR: 0.000226\n",
      "Train Epoch: 9 [13000/28539 (46%)]\tLoss: 2.688309\tLR: 0.000221\n",
      "Train Epoch: 9 [14000/28539 (49%)]\tLoss: 2.961876\tLR: 0.000216\n",
      "Train Epoch: 9 [15000/28539 (53%)]\tLoss: 2.633494\tLR: 0.000211\n",
      "Train Epoch: 9 [16000/28539 (56%)]\tLoss: 2.031650\tLR: 0.000206\n",
      "Train Epoch: 9 [17000/28539 (60%)]\tLoss: 2.407784\tLR: 0.000201\n",
      "Train Epoch: 9 [18000/28539 (63%)]\tLoss: 2.538212\tLR: 0.000196\n",
      "Train Epoch: 9 [19000/28539 (67%)]\tLoss: 2.450343\tLR: 0.000191\n",
      "Train Epoch: 9 [20000/28539 (70%)]\tLoss: 2.334124\tLR: 0.000186\n",
      "Train Epoch: 9 [21000/28539 (74%)]\tLoss: 2.306666\tLR: 0.000181\n",
      "Train Epoch: 9 [22000/28539 (77%)]\tLoss: 2.678594\tLR: 0.000175\n",
      "Train Epoch: 9 [23000/28539 (81%)]\tLoss: 2.324771\tLR: 0.000170\n",
      "Train Epoch: 9 [24000/28539 (84%)]\tLoss: 2.729183\tLR: 0.000165\n",
      "Train Epoch: 9 [25000/28539 (88%)]\tLoss: 2.498626\tLR: 0.000160\n",
      "Train Epoch: 9 [26000/28539 (91%)]\tLoss: 2.345629\tLR: 0.000155\n",
      "Train Epoch: 9 [27000/28539 (95%)]\tLoss: 2.789259\tLR: 0.000150\n",
      "Train Epoch: 9 [28000/28539 (98%)]\tLoss: 2.624018\tLR: 0.000145\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.1880, Average CER: 0.311325 Average WER: 0.5047\n",
      "\n",
      "Fri May  7 10:19:07 UTC 2021\n",
      "Train Epoch: 10 [0/28539 (0%)]\tLoss: 1.895864\tLR: 0.000143\n",
      "Train Epoch: 10 [1000/28539 (4%)]\tLoss: 2.196824\tLR: 0.000138\n",
      "Train Epoch: 10 [2000/28539 (7%)]\tLoss: 2.593959\tLR: 0.000133\n",
      "Train Epoch: 10 [3000/28539 (11%)]\tLoss: 2.389845\tLR: 0.000128\n",
      "Train Epoch: 10 [4000/28539 (14%)]\tLoss: 2.453335\tLR: 0.000123\n",
      "Train Epoch: 10 [5000/28539 (18%)]\tLoss: 2.338791\tLR: 0.000118\n",
      "Train Epoch: 10 [6000/28539 (21%)]\tLoss: 2.143666\tLR: 0.000113\n",
      "Train Epoch: 10 [7000/28539 (25%)]\tLoss: 2.335038\tLR: 0.000108\n",
      "Train Epoch: 10 [8000/28539 (28%)]\tLoss: 2.565367\tLR: 0.000103\n",
      "Train Epoch: 10 [9000/28539 (32%)]\tLoss: 2.470613\tLR: 0.000098\n",
      "Train Epoch: 10 [10000/28539 (35%)]\tLoss: 2.155467\tLR: 0.000093\n",
      "Train Epoch: 10 [11000/28539 (39%)]\tLoss: 2.427118\tLR: 0.000088\n",
      "Train Epoch: 10 [12000/28539 (42%)]\tLoss: 2.439887\tLR: 0.000083\n",
      "Train Epoch: 10 [13000/28539 (46%)]\tLoss: 2.463669\tLR: 0.000078\n",
      "Train Epoch: 10 [14000/28539 (49%)]\tLoss: 2.448161\tLR: 0.000073\n",
      "Train Epoch: 10 [15000/28539 (53%)]\tLoss: 2.458454\tLR: 0.000068\n",
      "Train Epoch: 10 [16000/28539 (56%)]\tLoss: 2.372650\tLR: 0.000063\n",
      "Train Epoch: 10 [17000/28539 (60%)]\tLoss: 2.547241\tLR: 0.000058\n",
      "Train Epoch: 10 [18000/28539 (63%)]\tLoss: 2.220612\tLR: 0.000053\n",
      "Train Epoch: 10 [19000/28539 (67%)]\tLoss: 2.314956\tLR: 0.000048\n",
      "Train Epoch: 10 [20000/28539 (70%)]\tLoss: 2.733977\tLR: 0.000043\n",
      "Train Epoch: 10 [21000/28539 (74%)]\tLoss: 2.512229\tLR: 0.000038\n",
      "Train Epoch: 10 [22000/28539 (77%)]\tLoss: 2.326283\tLR: 0.000033\n",
      "Train Epoch: 10 [23000/28539 (81%)]\tLoss: 2.306650\tLR: 0.000028\n",
      "Train Epoch: 10 [24000/28539 (84%)]\tLoss: 2.431602\tLR: 0.000023\n",
      "Train Epoch: 10 [25000/28539 (88%)]\tLoss: 2.269447\tLR: 0.000018\n",
      "Train Epoch: 10 [26000/28539 (91%)]\tLoss: 2.215664\tLR: 0.000013\n",
      "Train Epoch: 10 [27000/28539 (95%)]\tLoss: 2.115399\tLR: 0.000008\n",
      "Train Epoch: 10 [28000/28539 (98%)]\tLoss: 2.452403\tLR: 0.000003\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.1119, Average CER: 0.298179 Average WER: 0.4920\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 10\n",
    "test_batch_size = 7\n",
    "epochs = 10\n",
    "libri_train_set = \"train-clean-100\"\n",
    "libri_test_set = \"test-clean\"\n",
    "\n",
    "main(learning_rate, batch_size, test_batch_size, epochs, libri_train_set, libri_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cefuPY0bHL9"
   },
   "source": [
    "Результаты практически не изменились...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 967,
     "status": "ok",
     "timestamp": 1620394040019,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "7pgQ7DfcczIF"
   },
   "outputs": [],
   "source": [
    "#10 layers, increased dropout and learning rate\n",
    "def main(learning_rate=1e-5, batch_size=20, test_batch_size=7, epochs=10,\n",
    "        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
    "    \n",
    "    hparams = {\n",
    "        \"input_size\": 80,\n",
    "        \"output_size\": NUM_UNITS + 1,\n",
    "        \"conv2d_filters\": 32,\n",
    "        \"attention_dim\": 360,\n",
    "        \"attention_heads\": 8,\n",
    "        \"feedforward_dim\": 1024,\n",
    "        \"num_layers\":10,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if not os.path.isdir(\"./data\"):\n",
    "        os.makedirs(\"./data\")\n",
    "\n",
    "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
    "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=test_batch_size,\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                                **kwargs)\n",
    "\n",
    "    model = TransformerModel(\n",
    "        hparams['input_size'],\n",
    "        hparams['output_size'],\n",
    "        hparams['conv2d_filters'],\n",
    "        hparams['attention_dim'],\n",
    "        hparams['attention_heads'],\n",
    "        hparams['feedforward_dim'],\n",
    "        hparams['num_layers'],\n",
    "        hparams['dropout']).to(device)\n",
    "\n",
    "    print(model)\n",
    "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = torch.nn.CTCLoss(blank=BLANK, zero_infinity=False).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear',\n",
    "                                            pct_start=0.7)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        !date\n",
    "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
    "        if epoch % 3 == 0 or epoch % 10 == 0:\n",
    "            test(model, device, test_loader, criterion, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5950284,
     "status": "ok",
     "timestamp": 1620399993884,
     "user": {
      "displayName": "Pavel Karvenko",
      "photoUrl": "",
      "userId": "06247761077730525450"
     },
     "user_tz": -180
    },
    "id": "BiMR8vYQ3J7Y",
    "outputId": "2cb262b2-ec9c-4246-8e4e-e36f4c510df0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (conv_in): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_out): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=360, bias=True)\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layer): MultiSequential(\n",
      "    (0): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (1): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (2): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (3): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (4): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (5): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (6): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (7): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (8): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "    (9): EncoderLayer(\n",
      "      (self_attn): MultiHeadedAttention(\n",
      "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
      "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (concat_linear): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (after_norm): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
      "  (final_layer): Linear(in_features=360, out_features=4001, bias=True)\n",
      ")\n",
      "Num Model Parameters 14284849\n",
      "Fri May  7 13:27:24 UTC 2021\n",
      "Train Epoch: 1 [0/28539 (0%)]\tLoss: 52.891735\tLR: 0.000120\n",
      "Train Epoch: 1 [1000/28539 (4%)]\tLoss: 6.668804\tLR: 0.000135\n",
      "Train Epoch: 1 [2000/28539 (7%)]\tLoss: 6.837372\tLR: 0.000149\n",
      "Train Epoch: 1 [3000/28539 (11%)]\tLoss: 6.865322\tLR: 0.000163\n",
      "Train Epoch: 1 [4000/28539 (14%)]\tLoss: 6.959441\tLR: 0.000178\n",
      "Train Epoch: 1 [5000/28539 (18%)]\tLoss: 6.880082\tLR: 0.000192\n",
      "Train Epoch: 1 [6000/28539 (21%)]\tLoss: 6.872955\tLR: 0.000207\n",
      "Train Epoch: 1 [7000/28539 (25%)]\tLoss: 6.973940\tLR: 0.000221\n",
      "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 7.086286\tLR: 0.000235\n",
      "Train Epoch: 1 [9000/28539 (32%)]\tLoss: 6.856266\tLR: 0.000250\n",
      "Train Epoch: 1 [10000/28539 (35%)]\tLoss: 6.942604\tLR: 0.000264\n",
      "Train Epoch: 1 [11000/28539 (39%)]\tLoss: 6.930343\tLR: 0.000279\n",
      "Train Epoch: 1 [12000/28539 (42%)]\tLoss: 6.888088\tLR: 0.000293\n",
      "Train Epoch: 1 [13000/28539 (46%)]\tLoss: 6.819893\tLR: 0.000308\n",
      "Train Epoch: 1 [14000/28539 (49%)]\tLoss: 6.889720\tLR: 0.000322\n",
      "Train Epoch: 1 [15000/28539 (53%)]\tLoss: 6.747694\tLR: 0.000336\n",
      "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 6.814303\tLR: 0.000351\n",
      "Train Epoch: 1 [17000/28539 (60%)]\tLoss: 6.784603\tLR: 0.000365\n",
      "Train Epoch: 1 [18000/28539 (63%)]\tLoss: 6.905004\tLR: 0.000380\n",
      "Train Epoch: 1 [19000/28539 (67%)]\tLoss: 6.914697\tLR: 0.000394\n",
      "Train Epoch: 1 [20000/28539 (70%)]\tLoss: 6.620758\tLR: 0.000408\n",
      "Train Epoch: 1 [21000/28539 (74%)]\tLoss: 6.671338\tLR: 0.000423\n",
      "Train Epoch: 1 [22000/28539 (77%)]\tLoss: 6.792401\tLR: 0.000437\n",
      "Train Epoch: 1 [23000/28539 (81%)]\tLoss: 6.341899\tLR: 0.000452\n",
      "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 6.420827\tLR: 0.000466\n",
      "Train Epoch: 1 [25000/28539 (88%)]\tLoss: 6.388763\tLR: 0.000481\n",
      "Train Epoch: 1 [26000/28539 (91%)]\tLoss: 6.439441\tLR: 0.000495\n",
      "Train Epoch: 1 [27000/28539 (95%)]\tLoss: 6.380865\tLR: 0.000509\n",
      "Train Epoch: 1 [28000/28539 (98%)]\tLoss: 6.431720\tLR: 0.000524\n",
      "Fri May  7 13:35:36 UTC 2021\n",
      "Train Epoch: 2 [0/28539 (0%)]\tLoss: 6.129151\tLR: 0.000532\n",
      "Train Epoch: 2 [1000/28539 (4%)]\tLoss: 6.389174\tLR: 0.000546\n",
      "Train Epoch: 2 [2000/28539 (7%)]\tLoss: 6.338670\tLR: 0.000560\n",
      "Train Epoch: 2 [3000/28539 (11%)]\tLoss: 6.132357\tLR: 0.000575\n",
      "Train Epoch: 2 [4000/28539 (14%)]\tLoss: 6.207302\tLR: 0.000589\n",
      "Train Epoch: 2 [5000/28539 (18%)]\tLoss: 6.217371\tLR: 0.000604\n",
      "Train Epoch: 2 [6000/28539 (21%)]\tLoss: 5.890181\tLR: 0.000618\n",
      "Train Epoch: 2 [7000/28539 (25%)]\tLoss: 6.000320\tLR: 0.000633\n",
      "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 5.846102\tLR: 0.000647\n",
      "Train Epoch: 2 [9000/28539 (32%)]\tLoss: 5.795581\tLR: 0.000661\n",
      "Train Epoch: 2 [10000/28539 (35%)]\tLoss: 5.615051\tLR: 0.000676\n",
      "Train Epoch: 2 [11000/28539 (39%)]\tLoss: 5.760985\tLR: 0.000690\n",
      "Train Epoch: 2 [12000/28539 (42%)]\tLoss: 5.619033\tLR: 0.000705\n",
      "Train Epoch: 2 [13000/28539 (46%)]\tLoss: 5.427677\tLR: 0.000719\n",
      "Train Epoch: 2 [14000/28539 (49%)]\tLoss: 5.539783\tLR: 0.000733\n",
      "Train Epoch: 2 [15000/28539 (53%)]\tLoss: 5.530176\tLR: 0.000748\n",
      "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 5.866519\tLR: 0.000762\n",
      "Train Epoch: 2 [17000/28539 (60%)]\tLoss: 5.541750\tLR: 0.000777\n",
      "Train Epoch: 2 [18000/28539 (63%)]\tLoss: 5.648119\tLR: 0.000791\n",
      "Train Epoch: 2 [19000/28539 (67%)]\tLoss: 5.651389\tLR: 0.000806\n",
      "Train Epoch: 2 [20000/28539 (70%)]\tLoss: 5.429533\tLR: 0.000820\n",
      "Train Epoch: 2 [21000/28539 (74%)]\tLoss: 5.772485\tLR: 0.000834\n",
      "Train Epoch: 2 [22000/28539 (77%)]\tLoss: 5.290791\tLR: 0.000849\n",
      "Train Epoch: 2 [23000/28539 (81%)]\tLoss: 5.119833\tLR: 0.000863\n",
      "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 5.274826\tLR: 0.000878\n",
      "Train Epoch: 2 [25000/28539 (88%)]\tLoss: 5.279313\tLR: 0.000892\n",
      "Train Epoch: 2 [26000/28539 (91%)]\tLoss: 5.387742\tLR: 0.000906\n",
      "Train Epoch: 2 [27000/28539 (95%)]\tLoss: 5.312111\tLR: 0.000921\n",
      "Train Epoch: 2 [28000/28539 (98%)]\tLoss: 5.382221\tLR: 0.000935\n",
      "Fri May  7 13:43:49 UTC 2021\n",
      "Train Epoch: 3 [0/28539 (0%)]\tLoss: 4.804829\tLR: 0.000943\n",
      "Train Epoch: 3 [1000/28539 (4%)]\tLoss: 5.037395\tLR: 0.000957\n",
      "Train Epoch: 3 [2000/28539 (7%)]\tLoss: 5.115746\tLR: 0.000972\n",
      "Train Epoch: 3 [3000/28539 (11%)]\tLoss: 5.002171\tLR: 0.000986\n",
      "Train Epoch: 3 [4000/28539 (14%)]\tLoss: 5.395292\tLR: 0.001001\n",
      "Train Epoch: 3 [5000/28539 (18%)]\tLoss: 5.486200\tLR: 0.001015\n",
      "Train Epoch: 3 [6000/28539 (21%)]\tLoss: 5.064321\tLR: 0.001030\n",
      "Train Epoch: 3 [7000/28539 (25%)]\tLoss: 4.884290\tLR: 0.001044\n",
      "Train Epoch: 3 [8000/28539 (28%)]\tLoss: 5.125568\tLR: 0.001058\n",
      "Train Epoch: 3 [9000/28539 (32%)]\tLoss: 5.137399\tLR: 0.001073\n",
      "Train Epoch: 3 [10000/28539 (35%)]\tLoss: 4.991454\tLR: 0.001087\n",
      "Train Epoch: 3 [11000/28539 (39%)]\tLoss: 4.929088\tLR: 0.001102\n",
      "Train Epoch: 3 [12000/28539 (42%)]\tLoss: 5.229698\tLR: 0.001116\n",
      "Train Epoch: 3 [13000/28539 (46%)]\tLoss: 5.031816\tLR: 0.001130\n",
      "Train Epoch: 3 [14000/28539 (49%)]\tLoss: 5.188297\tLR: 0.001145\n",
      "Train Epoch: 3 [15000/28539 (53%)]\tLoss: 4.783935\tLR: 0.001159\n",
      "Train Epoch: 3 [16000/28539 (56%)]\tLoss: 5.394014\tLR: 0.001174\n",
      "Train Epoch: 3 [17000/28539 (60%)]\tLoss: 4.723640\tLR: 0.001188\n",
      "Train Epoch: 3 [18000/28539 (63%)]\tLoss: 5.080856\tLR: 0.001203\n",
      "Train Epoch: 3 [19000/28539 (67%)]\tLoss: 5.065624\tLR: 0.001217\n",
      "Train Epoch: 3 [20000/28539 (70%)]\tLoss: 5.058625\tLR: 0.001231\n",
      "Train Epoch: 3 [21000/28539 (74%)]\tLoss: 4.981797\tLR: 0.001246\n",
      "Train Epoch: 3 [22000/28539 (77%)]\tLoss: 5.230707\tLR: 0.001260\n",
      "Train Epoch: 3 [23000/28539 (81%)]\tLoss: 4.589151\tLR: 0.001275\n",
      "Train Epoch: 3 [24000/28539 (84%)]\tLoss: 4.930786\tLR: 0.001289\n",
      "Train Epoch: 3 [25000/28539 (88%)]\tLoss: 5.166200\tLR: 0.001303\n",
      "Train Epoch: 3 [26000/28539 (91%)]\tLoss: 4.848694\tLR: 0.001318\n",
      "Train Epoch: 3 [27000/28539 (95%)]\tLoss: 5.247301\tLR: 0.001332\n",
      "Train Epoch: 3 [28000/28539 (98%)]\tLoss: 5.130679\tLR: 0.001347\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 4.4524, Average CER: 0.753870 Average WER: 0.8352\n",
      "\n",
      "Fri May  7 13:54:28 UTC 2021\n",
      "Train Epoch: 4 [0/28539 (0%)]\tLoss: 4.961335\tLR: 0.001354\n",
      "Train Epoch: 4 [1000/28539 (4%)]\tLoss: 4.853538\tLR: 0.001369\n",
      "Train Epoch: 4 [2000/28539 (7%)]\tLoss: 5.057134\tLR: 0.001383\n",
      "Train Epoch: 4 [3000/28539 (11%)]\tLoss: 4.806980\tLR: 0.001398\n",
      "Train Epoch: 4 [4000/28539 (14%)]\tLoss: 4.873688\tLR: 0.001412\n",
      "Train Epoch: 4 [5000/28539 (18%)]\tLoss: 4.932098\tLR: 0.001427\n",
      "Train Epoch: 4 [6000/28539 (21%)]\tLoss: 4.684665\tLR: 0.001441\n",
      "Train Epoch: 4 [7000/28539 (25%)]\tLoss: 4.870872\tLR: 0.001455\n",
      "Train Epoch: 4 [8000/28539 (28%)]\tLoss: 4.620221\tLR: 0.001470\n",
      "Train Epoch: 4 [9000/28539 (32%)]\tLoss: 4.538556\tLR: 0.001484\n",
      "Train Epoch: 4 [10000/28539 (35%)]\tLoss: 4.463699\tLR: 0.001499\n",
      "Train Epoch: 4 [11000/28539 (39%)]\tLoss: 4.864903\tLR: 0.001513\n",
      "Train Epoch: 4 [12000/28539 (42%)]\tLoss: 4.515560\tLR: 0.001527\n",
      "Train Epoch: 4 [13000/28539 (46%)]\tLoss: 4.847553\tLR: 0.001542\n",
      "Train Epoch: 4 [14000/28539 (49%)]\tLoss: 4.316868\tLR: 0.001556\n",
      "Train Epoch: 4 [15000/28539 (53%)]\tLoss: 4.511963\tLR: 0.001571\n",
      "Train Epoch: 4 [16000/28539 (56%)]\tLoss: 4.409954\tLR: 0.001585\n",
      "Train Epoch: 4 [17000/28539 (60%)]\tLoss: 4.638650\tLR: 0.001600\n",
      "Train Epoch: 4 [18000/28539 (63%)]\tLoss: 4.812211\tLR: 0.001614\n",
      "Train Epoch: 4 [19000/28539 (67%)]\tLoss: 5.016438\tLR: 0.001628\n",
      "Train Epoch: 4 [20000/28539 (70%)]\tLoss: 4.823719\tLR: 0.001643\n",
      "Train Epoch: 4 [21000/28539 (74%)]\tLoss: 4.420403\tLR: 0.001657\n",
      "Train Epoch: 4 [22000/28539 (77%)]\tLoss: 4.498817\tLR: 0.001672\n",
      "Train Epoch: 4 [23000/28539 (81%)]\tLoss: 4.730263\tLR: 0.001686\n",
      "Train Epoch: 4 [24000/28539 (84%)]\tLoss: 4.295945\tLR: 0.001700\n",
      "Train Epoch: 4 [25000/28539 (88%)]\tLoss: 4.365469\tLR: 0.001715\n",
      "Train Epoch: 4 [26000/28539 (91%)]\tLoss: 4.498200\tLR: 0.001729\n",
      "Train Epoch: 4 [27000/28539 (95%)]\tLoss: 4.609575\tLR: 0.001744\n",
      "Train Epoch: 4 [28000/28539 (98%)]\tLoss: 4.645972\tLR: 0.001758\n",
      "Fri May  7 14:02:41 UTC 2021\n",
      "Train Epoch: 5 [0/28539 (0%)]\tLoss: 4.574800\tLR: 0.001766\n",
      "Train Epoch: 5 [1000/28539 (4%)]\tLoss: 4.296526\tLR: 0.001780\n",
      "Train Epoch: 5 [2000/28539 (7%)]\tLoss: 4.295484\tLR: 0.001795\n",
      "Train Epoch: 5 [3000/28539 (11%)]\tLoss: 4.474027\tLR: 0.001809\n",
      "Train Epoch: 5 [4000/28539 (14%)]\tLoss: 4.974431\tLR: 0.001824\n",
      "Train Epoch: 5 [5000/28539 (18%)]\tLoss: 4.659513\tLR: 0.001838\n",
      "Train Epoch: 5 [6000/28539 (21%)]\tLoss: 4.562925\tLR: 0.001852\n",
      "Train Epoch: 5 [7000/28539 (25%)]\tLoss: 4.481927\tLR: 0.001867\n",
      "Train Epoch: 5 [8000/28539 (28%)]\tLoss: 4.331693\tLR: 0.001881\n",
      "Train Epoch: 5 [9000/28539 (32%)]\tLoss: 4.377693\tLR: 0.001896\n",
      "Train Epoch: 5 [10000/28539 (35%)]\tLoss: 4.087688\tLR: 0.001910\n",
      "Train Epoch: 5 [11000/28539 (39%)]\tLoss: 4.153939\tLR: 0.001925\n",
      "Train Epoch: 5 [12000/28539 (42%)]\tLoss: 4.359684\tLR: 0.001939\n",
      "Train Epoch: 5 [13000/28539 (46%)]\tLoss: 4.311204\tLR: 0.001953\n",
      "Train Epoch: 5 [14000/28539 (49%)]\tLoss: 3.938754\tLR: 0.001968\n",
      "Train Epoch: 5 [15000/28539 (53%)]\tLoss: 4.583315\tLR: 0.001982\n",
      "Train Epoch: 5 [16000/28539 (56%)]\tLoss: 4.392040\tLR: 0.001997\n",
      "Train Epoch: 5 [17000/28539 (60%)]\tLoss: 4.238162\tLR: 0.002011\n",
      "Train Epoch: 5 [18000/28539 (63%)]\tLoss: 4.388878\tLR: 0.002025\n",
      "Train Epoch: 5 [19000/28539 (67%)]\tLoss: 4.353626\tLR: 0.002040\n",
      "Train Epoch: 5 [20000/28539 (70%)]\tLoss: 3.946082\tLR: 0.002054\n",
      "Train Epoch: 5 [21000/28539 (74%)]\tLoss: 4.590290\tLR: 0.002069\n",
      "Train Epoch: 5 [22000/28539 (77%)]\tLoss: 4.044673\tLR: 0.002083\n",
      "Train Epoch: 5 [23000/28539 (81%)]\tLoss: 4.188282\tLR: 0.002098\n",
      "Train Epoch: 5 [24000/28539 (84%)]\tLoss: 4.201202\tLR: 0.002112\n",
      "Train Epoch: 5 [25000/28539 (88%)]\tLoss: 4.380343\tLR: 0.002126\n",
      "Train Epoch: 5 [26000/28539 (91%)]\tLoss: 4.245187\tLR: 0.002141\n",
      "Train Epoch: 5 [27000/28539 (95%)]\tLoss: 4.727056\tLR: 0.002155\n",
      "Train Epoch: 5 [28000/28539 (98%)]\tLoss: 4.717046\tLR: 0.002170\n",
      "Fri May  7 14:10:53 UTC 2021\n",
      "Train Epoch: 6 [0/28539 (0%)]\tLoss: 4.452457\tLR: 0.002177\n",
      "Train Epoch: 6 [1000/28539 (4%)]\tLoss: 4.310630\tLR: 0.002192\n",
      "Train Epoch: 6 [2000/28539 (7%)]\tLoss: 4.291589\tLR: 0.002206\n",
      "Train Epoch: 6 [3000/28539 (11%)]\tLoss: 4.480900\tLR: 0.002221\n",
      "Train Epoch: 6 [4000/28539 (14%)]\tLoss: 4.093740\tLR: 0.002235\n",
      "Train Epoch: 6 [5000/28539 (18%)]\tLoss: 4.388578\tLR: 0.002249\n",
      "Train Epoch: 6 [6000/28539 (21%)]\tLoss: 4.292989\tLR: 0.002264\n",
      "Train Epoch: 6 [7000/28539 (25%)]\tLoss: 4.254915\tLR: 0.002278\n",
      "Train Epoch: 6 [8000/28539 (28%)]\tLoss: 4.673092\tLR: 0.002293\n",
      "Train Epoch: 6 [9000/28539 (32%)]\tLoss: 4.347034\tLR: 0.002307\n",
      "Train Epoch: 6 [10000/28539 (35%)]\tLoss: 4.027310\tLR: 0.002322\n",
      "Train Epoch: 6 [11000/28539 (39%)]\tLoss: 4.200637\tLR: 0.002336\n",
      "Train Epoch: 6 [12000/28539 (42%)]\tLoss: 4.283353\tLR: 0.002350\n",
      "Train Epoch: 6 [13000/28539 (46%)]\tLoss: 3.973376\tLR: 0.002365\n",
      "Train Epoch: 6 [14000/28539 (49%)]\tLoss: 4.058733\tLR: 0.002379\n",
      "Train Epoch: 6 [15000/28539 (53%)]\tLoss: 3.952375\tLR: 0.002394\n",
      "Train Epoch: 6 [16000/28539 (56%)]\tLoss: 3.958451\tLR: 0.002408\n",
      "Train Epoch: 6 [17000/28539 (60%)]\tLoss: 4.248438\tLR: 0.002422\n",
      "Train Epoch: 6 [18000/28539 (63%)]\tLoss: 4.436574\tLR: 0.002437\n",
      "Train Epoch: 6 [19000/28539 (67%)]\tLoss: 4.246464\tLR: 0.002451\n",
      "Train Epoch: 6 [20000/28539 (70%)]\tLoss: 4.401226\tLR: 0.002466\n",
      "Train Epoch: 6 [21000/28539 (74%)]\tLoss: 3.841955\tLR: 0.002480\n",
      "Train Epoch: 6 [22000/28539 (77%)]\tLoss: 3.663134\tLR: 0.002495\n",
      "Train Epoch: 6 [23000/28539 (81%)]\tLoss: 4.215140\tLR: 0.002509\n",
      "Train Epoch: 6 [24000/28539 (84%)]\tLoss: 4.214303\tLR: 0.002523\n",
      "Train Epoch: 6 [25000/28539 (88%)]\tLoss: 4.142323\tLR: 0.002538\n",
      "Train Epoch: 6 [26000/28539 (91%)]\tLoss: 3.744421\tLR: 0.002552\n",
      "Train Epoch: 6 [27000/28539 (95%)]\tLoss: 3.926618\tLR: 0.002567\n",
      "Train Epoch: 6 [28000/28539 (98%)]\tLoss: 4.085922\tLR: 0.002581\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 3.5586, Average CER: 0.548006 Average WER: 0.7128\n",
      "\n",
      "Fri May  7 14:23:14 UTC 2021\n",
      "Train Epoch: 7 [0/28539 (0%)]\tLoss: 4.202606\tLR: 0.002589\n",
      "Train Epoch: 7 [1000/28539 (4%)]\tLoss: 3.774952\tLR: 0.002603\n",
      "Train Epoch: 7 [2000/28539 (7%)]\tLoss: 3.810060\tLR: 0.002618\n",
      "Train Epoch: 7 [3000/28539 (11%)]\tLoss: 3.959054\tLR: 0.002632\n",
      "Train Epoch: 7 [4000/28539 (14%)]\tLoss: 4.049861\tLR: 0.002647\n",
      "Train Epoch: 7 [5000/28539 (18%)]\tLoss: 3.909909\tLR: 0.002661\n",
      "Train Epoch: 7 [6000/28539 (21%)]\tLoss: 3.963576\tLR: 0.002675\n",
      "Train Epoch: 7 [7000/28539 (25%)]\tLoss: 3.789233\tLR: 0.002690\n",
      "Train Epoch: 7 [8000/28539 (28%)]\tLoss: 3.685431\tLR: 0.002704\n",
      "Train Epoch: 7 [9000/28539 (32%)]\tLoss: 4.342718\tLR: 0.002719\n",
      "Train Epoch: 7 [10000/28539 (35%)]\tLoss: 4.347649\tLR: 0.002733\n",
      "Train Epoch: 7 [11000/28539 (39%)]\tLoss: 3.774778\tLR: 0.002747\n",
      "Train Epoch: 7 [12000/28539 (42%)]\tLoss: 3.830624\tLR: 0.002762\n",
      "Train Epoch: 7 [13000/28539 (46%)]\tLoss: 4.272480\tLR: 0.002776\n",
      "Train Epoch: 7 [14000/28539 (49%)]\tLoss: 3.618184\tLR: 0.002791\n",
      "Train Epoch: 7 [15000/28539 (53%)]\tLoss: 3.870423\tLR: 0.002805\n",
      "Train Epoch: 7 [16000/28539 (56%)]\tLoss: 3.682478\tLR: 0.002820\n",
      "Train Epoch: 7 [17000/28539 (60%)]\tLoss: 3.917268\tLR: 0.002834\n",
      "Train Epoch: 7 [18000/28539 (63%)]\tLoss: 3.704894\tLR: 0.002848\n",
      "Train Epoch: 7 [19000/28539 (67%)]\tLoss: 3.863198\tLR: 0.002863\n",
      "Train Epoch: 7 [20000/28539 (70%)]\tLoss: 3.802166\tLR: 0.002877\n",
      "Train Epoch: 7 [21000/28539 (74%)]\tLoss: 3.726603\tLR: 0.002892\n",
      "Train Epoch: 7 [22000/28539 (77%)]\tLoss: 3.683429\tLR: 0.002906\n",
      "Train Epoch: 7 [23000/28539 (81%)]\tLoss: 3.681229\tLR: 0.002920\n",
      "Train Epoch: 7 [24000/28539 (84%)]\tLoss: 3.750941\tLR: 0.002935\n",
      "Train Epoch: 7 [25000/28539 (88%)]\tLoss: 3.790184\tLR: 0.002949\n",
      "Train Epoch: 7 [26000/28539 (91%)]\tLoss: 3.678200\tLR: 0.002964\n",
      "Train Epoch: 7 [27000/28539 (95%)]\tLoss: 4.130105\tLR: 0.002978\n",
      "Train Epoch: 7 [28000/28539 (98%)]\tLoss: 3.848334\tLR: 0.002993\n",
      "Fri May  7 14:31:26 UTC 2021\n",
      "Train Epoch: 8 [0/28539 (0%)]\tLoss: 3.911710\tLR: 0.002999\n",
      "Train Epoch: 8 [1000/28539 (4%)]\tLoss: 3.752911\tLR: 0.002964\n",
      "Train Epoch: 8 [2000/28539 (7%)]\tLoss: 3.099827\tLR: 0.002929\n",
      "Train Epoch: 8 [3000/28539 (11%)]\tLoss: 3.726910\tLR: 0.002894\n",
      "Train Epoch: 8 [4000/28539 (14%)]\tLoss: 3.651264\tLR: 0.002859\n",
      "Train Epoch: 8 [5000/28539 (18%)]\tLoss: 3.784971\tLR: 0.002824\n",
      "Train Epoch: 8 [6000/28539 (21%)]\tLoss: 3.601444\tLR: 0.002789\n",
      "Train Epoch: 8 [7000/28539 (25%)]\tLoss: 3.531615\tLR: 0.002754\n",
      "Train Epoch: 8 [8000/28539 (28%)]\tLoss: 3.657655\tLR: 0.002719\n",
      "Train Epoch: 8 [9000/28539 (32%)]\tLoss: 3.409370\tLR: 0.002684\n",
      "Train Epoch: 8 [10000/28539 (35%)]\tLoss: 3.736675\tLR: 0.002649\n",
      "Train Epoch: 8 [11000/28539 (39%)]\tLoss: 3.299769\tLR: 0.002614\n",
      "Train Epoch: 8 [12000/28539 (42%)]\tLoss: 3.497602\tLR: 0.002579\n",
      "Train Epoch: 8 [13000/28539 (46%)]\tLoss: 4.214799\tLR: 0.002544\n",
      "Train Epoch: 8 [14000/28539 (49%)]\tLoss: 3.793272\tLR: 0.002509\n",
      "Train Epoch: 8 [15000/28539 (53%)]\tLoss: 3.495090\tLR: 0.002474\n",
      "Train Epoch: 8 [16000/28539 (56%)]\tLoss: 3.382324\tLR: 0.002439\n",
      "Train Epoch: 8 [17000/28539 (60%)]\tLoss: 3.550721\tLR: 0.002404\n",
      "Train Epoch: 8 [18000/28539 (63%)]\tLoss: 3.314894\tLR: 0.002369\n",
      "Train Epoch: 8 [19000/28539 (67%)]\tLoss: 3.414786\tLR: 0.002334\n",
      "Train Epoch: 8 [20000/28539 (70%)]\tLoss: 2.999017\tLR: 0.002299\n",
      "Train Epoch: 8 [21000/28539 (74%)]\tLoss: 3.755023\tLR: 0.002263\n",
      "Train Epoch: 8 [22000/28539 (77%)]\tLoss: 3.490955\tLR: 0.002228\n",
      "Train Epoch: 8 [23000/28539 (81%)]\tLoss: 3.433352\tLR: 0.002193\n",
      "Train Epoch: 8 [24000/28539 (84%)]\tLoss: 3.427420\tLR: 0.002158\n",
      "Train Epoch: 8 [25000/28539 (88%)]\tLoss: 3.246023\tLR: 0.002123\n",
      "Train Epoch: 8 [26000/28539 (91%)]\tLoss: 3.360530\tLR: 0.002088\n",
      "Train Epoch: 8 [27000/28539 (95%)]\tLoss: 3.178196\tLR: 0.002053\n",
      "Train Epoch: 8 [28000/28539 (98%)]\tLoss: 3.482132\tLR: 0.002018\n",
      "Fri May  7 14:39:37 UTC 2021\n",
      "Train Epoch: 9 [0/28539 (0%)]\tLoss: 3.315235\tLR: 0.001999\n",
      "Train Epoch: 9 [1000/28539 (4%)]\tLoss: 3.502834\tLR: 0.001964\n",
      "Train Epoch: 9 [2000/28539 (7%)]\tLoss: 3.147344\tLR: 0.001929\n",
      "Train Epoch: 9 [3000/28539 (11%)]\tLoss: 3.329316\tLR: 0.001894\n",
      "Train Epoch: 9 [4000/28539 (14%)]\tLoss: 3.471743\tLR: 0.001859\n",
      "Train Epoch: 9 [5000/28539 (18%)]\tLoss: 3.130604\tLR: 0.001824\n",
      "Train Epoch: 9 [6000/28539 (21%)]\tLoss: 2.874012\tLR: 0.001789\n",
      "Train Epoch: 9 [7000/28539 (25%)]\tLoss: 3.150087\tLR: 0.001754\n",
      "Train Epoch: 9 [8000/28539 (28%)]\tLoss: 2.965191\tLR: 0.001719\n",
      "Train Epoch: 9 [9000/28539 (32%)]\tLoss: 3.247916\tLR: 0.001684\n",
      "Train Epoch: 9 [10000/28539 (35%)]\tLoss: 3.177255\tLR: 0.001649\n",
      "Train Epoch: 9 [11000/28539 (39%)]\tLoss: 3.472485\tLR: 0.001614\n",
      "Train Epoch: 9 [12000/28539 (42%)]\tLoss: 2.659269\tLR: 0.001579\n",
      "Train Epoch: 9 [13000/28539 (46%)]\tLoss: 3.242661\tLR: 0.001544\n",
      "Train Epoch: 9 [14000/28539 (49%)]\tLoss: 2.922137\tLR: 0.001509\n",
      "Train Epoch: 9 [15000/28539 (53%)]\tLoss: 3.327942\tLR: 0.001474\n",
      "Train Epoch: 9 [16000/28539 (56%)]\tLoss: 3.072540\tLR: 0.001439\n",
      "Train Epoch: 9 [17000/28539 (60%)]\tLoss: 3.121783\tLR: 0.001404\n",
      "Train Epoch: 9 [18000/28539 (63%)]\tLoss: 2.986395\tLR: 0.001369\n",
      "Train Epoch: 9 [19000/28539 (67%)]\tLoss: 3.202635\tLR: 0.001334\n",
      "Train Epoch: 9 [20000/28539 (70%)]\tLoss: 3.209104\tLR: 0.001299\n",
      "Train Epoch: 9 [21000/28539 (74%)]\tLoss: 2.890419\tLR: 0.001263\n",
      "Train Epoch: 9 [22000/28539 (77%)]\tLoss: 2.959920\tLR: 0.001228\n",
      "Train Epoch: 9 [23000/28539 (81%)]\tLoss: 2.833742\tLR: 0.001193\n",
      "Train Epoch: 9 [24000/28539 (84%)]\tLoss: 3.291750\tLR: 0.001158\n",
      "Train Epoch: 9 [25000/28539 (88%)]\tLoss: 3.064170\tLR: 0.001123\n",
      "Train Epoch: 9 [26000/28539 (91%)]\tLoss: 2.916731\tLR: 0.001088\n",
      "Train Epoch: 9 [27000/28539 (95%)]\tLoss: 2.975189\tLR: 0.001053\n",
      "Train Epoch: 9 [28000/28539 (98%)]\tLoss: 3.172764\tLR: 0.001018\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.4793, Average CER: 0.370881 Average WER: 0.5583\n",
      "\n",
      "Fri May  7 14:53:01 UTC 2021\n",
      "Train Epoch: 10 [0/28539 (0%)]\tLoss: 3.007563\tLR: 0.000999\n",
      "Train Epoch: 10 [1000/28539 (4%)]\tLoss: 2.706421\tLR: 0.000964\n",
      "Train Epoch: 10 [2000/28539 (7%)]\tLoss: 2.682935\tLR: 0.000929\n",
      "Train Epoch: 10 [3000/28539 (11%)]\tLoss: 3.057959\tLR: 0.000894\n",
      "Train Epoch: 10 [4000/28539 (14%)]\tLoss: 2.575013\tLR: 0.000859\n",
      "Train Epoch: 10 [5000/28539 (18%)]\tLoss: 2.410044\tLR: 0.000824\n",
      "Train Epoch: 10 [6000/28539 (21%)]\tLoss: 2.921882\tLR: 0.000789\n",
      "Train Epoch: 10 [7000/28539 (25%)]\tLoss: 2.609608\tLR: 0.000754\n",
      "Train Epoch: 10 [8000/28539 (28%)]\tLoss: 2.849039\tLR: 0.000719\n",
      "Train Epoch: 10 [9000/28539 (32%)]\tLoss: 2.538897\tLR: 0.000684\n",
      "Train Epoch: 10 [10000/28539 (35%)]\tLoss: 2.696598\tLR: 0.000649\n",
      "Train Epoch: 10 [11000/28539 (39%)]\tLoss: 2.484930\tLR: 0.000614\n",
      "Train Epoch: 10 [12000/28539 (42%)]\tLoss: 2.449906\tLR: 0.000579\n",
      "Train Epoch: 10 [13000/28539 (46%)]\tLoss: 2.487477\tLR: 0.000544\n",
      "Train Epoch: 10 [14000/28539 (49%)]\tLoss: 2.910308\tLR: 0.000509\n",
      "Train Epoch: 10 [15000/28539 (53%)]\tLoss: 2.792802\tLR: 0.000474\n",
      "Train Epoch: 10 [16000/28539 (56%)]\tLoss: 2.556267\tLR: 0.000439\n",
      "Train Epoch: 10 [17000/28539 (60%)]\tLoss: 2.313609\tLR: 0.000404\n",
      "Train Epoch: 10 [18000/28539 (63%)]\tLoss: 2.762102\tLR: 0.000369\n",
      "Train Epoch: 10 [19000/28539 (67%)]\tLoss: 2.575120\tLR: 0.000334\n",
      "Train Epoch: 10 [20000/28539 (70%)]\tLoss: 2.644945\tLR: 0.000299\n",
      "Train Epoch: 10 [21000/28539 (74%)]\tLoss: 2.774794\tLR: 0.000264\n",
      "Train Epoch: 10 [22000/28539 (77%)]\tLoss: 2.733089\tLR: 0.000228\n",
      "Train Epoch: 10 [23000/28539 (81%)]\tLoss: 2.555717\tLR: 0.000193\n",
      "Train Epoch: 10 [24000/28539 (84%)]\tLoss: 2.973729\tLR: 0.000158\n",
      "Train Epoch: 10 [25000/28539 (88%)]\tLoss: 2.586532\tLR: 0.000123\n",
      "Train Epoch: 10 [26000/28539 (91%)]\tLoss: 2.694149\tLR: 0.000088\n",
      "Train Epoch: 10 [27000/28539 (95%)]\tLoss: 2.733019\tLR: 0.000053\n",
      "Train Epoch: 10 [28000/28539 (98%)]\tLoss: 2.634188\tLR: 0.000018\n",
      "\n",
      "evaluating...\n",
      "Test set: Average loss: 2.2834, Average CER: 0.335365 Average WER: 0.5299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-3\n",
    "batch_size = 10\n",
    "test_batch_size = 7\n",
    "epochs = 10\n",
    "libri_train_set = \"train-clean-100\"\n",
    "libri_test_set = \"test-clean\"\n",
    "\n",
    "main(learning_rate, batch_size, test_batch_size, epochs, libri_train_set, libri_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHfl_ZzO3T2r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of asr_lab_4_new.ipynb",
   "provenance": [
    {
     "file_id": "1KTHcL0NaSTOPud24nileeitWSKyUzzmv",
     "timestamp": 1620295645907
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "133f24e923fe4dd29fce17737471b515": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "188c8e41fb8b43cc8931193271d300be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dbf84fb63b8c4b438cef90b4328a4502",
       "IPY_MODEL_53064c0e94134dddba7dac19c24a6b6c"
      ],
      "layout": "IPY_MODEL_531501918e52470dbc06ed1bcd91f547"
     }
    },
    "53064c0e94134dddba7dac19c24a6b6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_133f24e923fe4dd29fce17737471b515",
      "placeholder": "​",
      "style": "IPY_MODEL_d6d9a6a758244ce9a2b5341d584cae80",
      "value": " 5.95G/5.95G [10:42&lt;00:00, 9.95MB/s]"
     }
    },
    "531501918e52470dbc06ed1bcd91f547": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ed8eba52718496da1c9bfd1ce765cb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7070e1b844c44b52a0ff5c748d0eaf61": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "89f11cc02fc044b2a37ab6c0303dbcda": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90bab181629a417fbe73ed2f547bbe9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89f11cc02fc044b2a37ab6c0303dbcda",
      "placeholder": "​",
      "style": "IPY_MODEL_5ed8eba52718496da1c9bfd1ce765cb0",
      "value": " 331M/331M [00:31&lt;00:00, 11.0MB/s]"
     }
    },
    "964f1d0b0efa47b2ab21fd338e989409": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bc762a242d648ddba060f04929563fc",
      "max": 346663984,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7070e1b844c44b52a0ff5c748d0eaf61",
      "value": 346663984
     }
    },
    "9bc762a242d648ddba060f04929563fc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bc684a77a490426e9758d117cefe722c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c92aacae1e5449c5a0d9ceea7ce0e01b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d3a6f08599274787ae924cf8d011e3e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_964f1d0b0efa47b2ab21fd338e989409",
       "IPY_MODEL_90bab181629a417fbe73ed2f547bbe9f"
      ],
      "layout": "IPY_MODEL_bc684a77a490426e9758d117cefe722c"
     }
    },
    "d6d9a6a758244ce9a2b5341d584cae80": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dbf84fb63b8c4b438cef90b4328a4502": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df1c0693de1f4d8bbb8bd5a67be639fd",
      "max": 6387309499,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c92aacae1e5449c5a0d9ceea7ce0e01b",
      "value": 6387309499
     }
    },
    "df1c0693de1f4d8bbb8bd5a67be639fd": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
