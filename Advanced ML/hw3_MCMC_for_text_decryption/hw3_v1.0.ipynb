{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "divine-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-founder",
   "metadata": {},
   "source": [
    "Продвинутое машинное обучение: \n",
    "Домашнее задание 3\n",
    "Третье домашнее задание посвящено достаточно простой, но, надеюсь, интересной задаче, в которой потребуется творчески применить методы сэмплирования. Как и раньше, в качестве решения ожидается ссылка на jupyter-ноутбук на вашем github (или публичный, или с доступом для snikolenko); ссылку обязательно нужно прислать в виде сданного домашнего задания на портале Академии. Как всегда, любые комментарии, новые идеи и рассуждения на тему категорически приветствуются. \n",
    "В этом небольшом домашнем задании мы попробуем улучшить метод Шерлока Холмса. Как известно, в рассказе The Adventure of the Dancing Men великий сыщик расшифровал загадочные письмена, которые выглядели примерно так:\n",
    "\n",
    "Пользовался он для этого так называемым частотным методом: смотрел, какие буквы чаще встречаются в зашифрованных текстах, и пытался подставить буквы в соответствии с частотной таблицей: E — самая частая и так далее.\n",
    "В этом задании мы будем разрабатывать более современный и продвинутый вариант такого частотного метода. В качестве корпусов текстов для подсчётов частот можете взять что угодно, но для удобства вот вам “Война и мир” по-русски и по-английски:\n",
    "https://www.dropbox.com/s/k23enjvr3fb40o5/corpora.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-length",
   "metadata": {},
   "source": [
    "## 1. Реализуйте базовый частотный метод по Шерлоку Холмсу:\n",
    "подсчитайте частоты букв по корпусам (пунктуацию и капитализацию можно просто опустить, а вот пробелы лучше оставить);\n",
    "возьмите какие-нибудь тестовые тексты (нужно взять по меньшей мере 2-3 предложения, иначе вряд ли сработает), зашифруйте их посредством случайной перестановки символов;\n",
    "расшифруйте их таким частотным методом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "innovative-nelson",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_FILE = 'AnnaKarenina.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "alien-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_TEXT = 'დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭჾႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭჾႣჵისႼჰႨჲდႩჳჲႨჇႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵჇႧჂჲდႨჾႣႩჳჂჾႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხსდდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩჇႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "accomplished-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_encode_dict(symbols):\n",
    "    \"\"\"Generates random permutation dict based on list of symbols\"\"\"\n",
    "    encoded = symbols.copy()\n",
    "    random.shuffle(encoded)\n",
    "    \n",
    "    result = {}\n",
    "    for o, e in zip(symbols, encoded):\n",
    "        result[o] = e\n",
    "    return result\n",
    "\n",
    "def make_decode_dict(e_dict):\n",
    "    \"\"\"Makes decode dict based on encode dict\"\"\"\n",
    "    result = {}\n",
    "    for k, v in e_dict.items():\n",
    "        result[v] = k\n",
    "    return result\n",
    "\n",
    "def encode_str(text, e_dict):\n",
    "    \"\"\"Encodes text with encode dict. For decode - just pass decode dict\"\"\"\n",
    "    result = str()\n",
    "    for c in text:\n",
    "        result += e_dict[c]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "described-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return re.sub(r\"\\W+\", \" \", ''.join(text)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "equipped-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making corpus\n",
    "with open(CORPUS_FILE, 'r') as f:\n",
    "    corpus = f.readlines()\n",
    "    \n",
    "corpus = preprocess_text(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "greatest-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_list(text):\n",
    "    c = Counter(text)\n",
    "    dl = len(text)\n",
    "    result = []\n",
    "    for k, v in c.items():\n",
    "        result.append((k, v / dl))\n",
    "    result = sorted(result, key=lambda x: x[1], reverse=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "disabled-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_decode_dict(corp_freq, text_freq, method='rank'):\n",
    "    \"\"\"Generates decode dictionary based on corpus freq & text freq.\n",
    "    Has two methods - 'rank' - 1st to 1st, 2nd to 2nd etc, and 'closest' - finds closest freq. \n",
    "    Last method can map several tokens to one\"\"\"\n",
    "    result = {}\n",
    "    if method == 'rank':\n",
    "        for i in range(min(len(corp_freq), len(text_freq))):\n",
    "            result[text_freq[i][0]] = corp_freq[i][0]\n",
    "    elif method == 'closest':\n",
    "        for c, prob in text_freq:\n",
    "            best_dist = 10\n",
    "            best_char = None\n",
    "            for cc, pp in corp_freq:\n",
    "                if abs(pp - prob) < best_dist:\n",
    "                    best_dist = abs(pp - prob)\n",
    "                    best_char = cc\n",
    "            result[c] = best_char\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "imperial-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_dict_quality(enc_dict, dec_dict):\n",
    "    \"\"\"Counts correct replacements\"\"\"\n",
    "    counter = 0\n",
    "    for a, b in enc_dict.items():\n",
    "        try:\n",
    "            if dec_dict[b] == a:\n",
    "                counter += 1\n",
    "        except:\n",
    "            pass\n",
    "    return counter / len(enc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dental-indicator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode(text, corpus, method='closest'):\n",
    "    \"\"\"Encodes text with random permutation and then decodes it using corpus\"\"\"\n",
    "    text = preprocess_text(text)\n",
    "    enc_dict = make_encode_dict(list(set(text)))\n",
    "    enc_text = encode_str(text, enc_dict)\n",
    "    corp_freq = get_freq_list(corpus)\n",
    "    enc_freq = get_freq_list(enc_text)\n",
    "    dec_dict = find_decode_dict(corp_freq, enc_freq, method)\n",
    "    print(encode_str(enc_text, dec_dict))\n",
    "    print(text)\n",
    "    print('Dict quality: ', measure_dict_quality(enc_dict, dec_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "regular-crest",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text1 = 'Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "advisory-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text2 = preprocess_text('Быть энтузиасткой сделалось ее общественным положением, и иногда, когда ей даже того не хотелось, она, чтобы не обмануть ожиданий людей, знавших ее, делалась энтузиасткой. Сдержанная улыбка, игравшая постоянно на лице Анны Павловны, хотя и не шла к ее отжившим чертам, выражала, как у избалованных детей, постоянное сознание своего милого недостатка, от которого она не хочет, не может и не находит нужным исправляться.В середине разговора про политические действия Анна Павловна разгорячилась.— Ах, не говорите мне про Австрию! Я ничего не понимаю, может быть, но Австрия никогда не хотела и не хочет войны. Она предает нас. Россия одна должна быть спасительницей Европы. Наш благодетель знает свое высокое призвание и будет верен ему. Вот одно, во что я верю. Нашему доброму и чудному государю предстоит величайшая роль в мире, и он так добродетелен и хорош, что Бог не оставит его, и он исполнит свое призвание задавить гидру революции, которая теперь еще ужаснее в лице этого убийцы и злодея. Мы одни должны искупить кровь праведника На кого нам надеяться, я вас спрашиваю? Англия с своим коммерческим духом не поймет и не может понять всю высоту души императора Александра. Она отказалась очистить Мальту. Она хочет видеть, ищет заднюю мысль наших действий. Что они сказали Новосильцову? Ничего. Они не поняли, они не могут понять самоотвержения нашего императора, который ничего не хочет для себя и всё хочет для блага мира. И что они обещали? Ничего. И что обещали, и того не будет! Пруссия уж объявила, что Бонапарте непобедим и что вся Европа ничего не может против него И я не верю ни в одном слове ни Гарденбергу, ни Гаугвицу. Cette fameuse neutralité prussienne, ce nest quun piège. Этот пресловутый нейтралитет Пруссии — только западня. Я верю в одного Бога и в высокую судьбу нашего милого императора. Он спасет Европу! — Она вдруг остановилась с улыбкою насмешки над своею горячностью.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "typical-alexander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "влпу ии в лньуиьаоан баиубииось аолоп уш жалазоп лосзищлавло лоьвн баа нсии вр злоии в лобнсавн ансааврж уорррж щнирн лосслоьр ра и знлиал жаибс зри рн аол уш бласа басин зулвр щ лааалоп уньсавианиьра врунипнасп босааааь уоиьзн ар ороииьилавои ушн ларллнаррн сиаво и браоисп узоуоаь лолиби ари базии зр зраь п рн ьрою лол ьоблазлоббилавоаь аолаь ирауиаивррь ороииь аол баа уовоьан бласаа сунионб синуующиь иазибнслиь зоз\n",
      "вряд ли в результате получилась такая уж хорошая расшифровка разве что если вы брали в качестве тестовых данных целые рассказы но и шерлок холмс был не так уж прост после буквы e которая действительно выделяется частотой дальше он анализировал уже конкретные слова и пытался угадать какими они могли бы быть я не знаю как запрограммировать такой интуитивный анализ так что давайте просто сделаем следующий логический шаг\n",
      "Dict quality:  0.25\n"
     ]
    }
   ],
   "source": [
    "encode_decode(test_text1, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "portable-silicon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "чзты цатпйнавтгож вкекаковы ее очэевтреаазп покойеанеп н наоука гоука еж кайе тоуо ае йотековы оаа чточз ае очпаапты ойнкаанж кйкеж йааршнй ее кекакавы цатпйнавтгож вкерйаааая пкзчга нураршая повтояаао аа кнэе аааз паркораз йотя н ае шка г ее отйнршнп чертап рзрайака гаг п нйчакорааазй кетеж повтояааое войааане вроеуо пнкоуо аековтатга от готороуо оаа ае йочет ае пойет н ае аайокнт апйазп нвпраркятывя р верекнае райуорора про покнтнчевгне кежвтрня аааа паркораа райуорячнкавы ай ае уорорнте пае про арвтрнй я анчеуо ае поанпай пойет чзты ао арвтрня ангоука ае йотека н ае йочет рожаз оаа прекает аав роввня окаа кокйаа чзты впавнтекыанэеж ерропз ааш чкауокетекы йаает врое рзвогое прнйраане н чпкет ререа епп рот окао ро что я рерй аашепп кочропп н чпкаопп уовпкарй преквтонт рекнчажшая рокы р пнре н оа таг кочрокетекеа н йорош что чоу ае овтарнт еуо н оа нвпокант врое прнйраане йакарнты ункрп рерокйэнн готорая теперы еэе пйаваее р кнэе цтоуо пчнжэз н йкокея пз окан кокйаз нвгппнты гроры прареканга аа гоуо аап аакеятывя я рав впрашнрай ааукня в вронп гопперчевгнп кпйоп ае пожпет н ае пойет поаяты рвй рзвотп кпшн нпператора акегваакра оаа отгайакавы очнвтнты пакытп оаа йочет рнкеты нэет йакайй пзвкы аашнй кежвтрнж что оан вгайакн аоровнкыэорп анчеуо оан ае поаякн оан ае поупт поаяты вапоотрерйеаня аашеуо нпператора готорзж анчеуо ае йочет ккя вечя н рв1 йочет ккя чкауа пнра н что оан очеэакн анчеуо н что очеэакн н тоуо ае чпкет прпввня пй оч1ярнка что чоаапарте аепочекнп н что рвя ерропа анчеуо ае пойет протнр аеуо н я ае рерй ан р окаоп вкоре ан уаркеачеруп ан уапурнэп фюээю 1ф1юэцю эюээфф1фэ1 ффэццфюээю фю эюцэ 1эээ фф11ю цтот превкорптзж аежтракнтет прпввнн токыго йапакая я рерй р окаоуо чоуа н р рзвогпй впкычп аашеуо пнкоуо нпператора оа впавет ерропп оаа ркрпу овтааорнкавы в пкзчгой аавпешгн аак вроей уорячаовтый \n",
      "быть энтузиасткой сделалось ее общественным положением и иногда когда ей даже того не хотелось она чтобы не обмануть ожиданий людей знавших ее делалась энтузиасткой сдержанная улыбка игравшая постоянно на лице анны павловны хотя и не шла к ее отжившим чертам выражала как у избалованных детей постоянное сознание своего милого недостатка от которого она не хочет не может и не находит нужным исправляться в середине разговора про политические действия анна павловна разгорячилась ах не говорите мне про австрию я ничего не понимаю может быть но австрия никогда не хотела и не хочет войны она предает нас россия одна должна быть спасительницей европы наш благодетель знает свое высокое призвание и будет верен ему вот одно во что я верю нашему доброму и чудному государю предстоит величайшая роль в мире и он так добродетелен и хорош что бог не оставит его и он исполнит свое призвание задавить гидру революции которая теперь еще ужаснее в лице этого убийцы и злодея мы одни должны искупить кровь праведника на кого нам надеяться я вас спрашиваю англия с своим коммерческим духом не поймет и не может понять всю высоту души императора александра она отказалась очистить мальту она хочет видеть ищет заднюю мысль наших действий что они сказали новосильцову ничего они не поняли они не могут понять самоотвержения нашего императора который ничего не хочет для себя и всё хочет для блага мира и что они обещали ничего и что обещали и того не будет пруссия уж объявила что бонапарте непобедим и что вся европа ничего не может против него и я не верю ни в одном слове ни гарденбергу ни гаугвицу cette fameuse neutralité prussienne ce nest quun piège этот пресловутый нейтралитет пруссии только западня я верю в одного бога и в высокую судьбу нашего милого императора он спасет европу она вдруг остановилась с улыбкою насмешки над своею горячностью \n",
      "Dict quality:  0.2\n"
     ]
    }
   ],
   "source": [
    "encode_decode(test_text2, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "square-fifteen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "гчть iнтпжиастыоз скерарось ее огцестленнчм уоройением и инодка ыодка ез кайе тодо не хотерось она бтогч не огманпть ойиканиз ршкез жналюих ее керарась iнтпжиастыоз скевйанная прчгыа идвалюая уостоянно на рище аннч уалролнч хотя и не юра ы ее отйилюим бевтам лчвайара ыаы п ижгароланнчх кетез уостоянное сожнание слоедо миродо некостатыа от ыотоводо она не хобет не мойет и не нахокит нпйнчм исувалряться л севекине важдолова уво уоритибесыие кезстлия анна уалролна важдовябирась ах не доловите мне уво алствиш я нибедо не уонимаш мойет гчть но алствия ниыодка не хотера и не хобет лознч она увекает нас воссия окна корйна гчть суаситерьнищез елвоуч наю градокетерь жнает слое лчсоыое увижлание и гпкет левен емп лот окно ло бто я левш наюемп когвомп и бпкномп доспкавш увекстоит лерибазюая ворь л миве и он таы когвокетерен и ховою бто год не осталит едо и он исуорнит слое увижлание жакалить диквп велоршщии ыотовая теуевь еце пйаснее л рище iтодо пгизщч и жрокея мч окни корйнч исыпуить ыволь увалекниыа на ыодо нам накеяться я лас суваюилаш андрия с слоим ыоммевбесыим кпхом не уозмет и не мойет уонять лсш лчсотп кпюи имуеватова ареысанква она отыажарась обистить марьтп она хобет ликеть ицет жакншш мчсрь наюих кезстлиз бто они сыажари нолосирьщолп нибедо они не уоняри они не модпт уонять самоотлевйения наюедо имуеватова ыотовчз нибедо не хобет кря сегя и лс3 хобет кря града мива и бто они огецари нибедо и бто огецари и тодо не гпкет увпссия пй огxялира бто гонауавте неуогеким и бто лся елвоуа нибедо не мойет увотил недо и я не левш ни л окном сроле ни давкенгевдп ни дапдлищп 8эффэ lъ7э1aэ eэ1фnъr2фt sn1aa2эeeэ 8э eэaф o11e s2m6э iтот увесролптчз незтваритет увпссии торьыо жауакня я левш л окнодо года и л лчсоыпш спкьгп наюедо миродо имуеватова он суасет елвоуп она лквпд останолирась с прчгыош насмеюыи нак слоеш довябностьш \n",
      "быть энтузиасткой сделалось ее общественным положением и иногда когда ей даже того не хотелось она чтобы не обмануть ожиданий людей знавших ее делалась энтузиасткой сдержанная улыбка игравшая постоянно на лице анны павловны хотя и не шла к ее отжившим чертам выражала как у избалованных детей постоянное сознание своего милого недостатка от которого она не хочет не может и не находит нужным исправляться в середине разговора про политические действия анна павловна разгорячилась ах не говорите мне про австрию я ничего не понимаю может быть но австрия никогда не хотела и не хочет войны она предает нас россия одна должна быть спасительницей европы наш благодетель знает свое высокое призвание и будет верен ему вот одно во что я верю нашему доброму и чудному государю предстоит величайшая роль в мире и он так добродетелен и хорош что бог не оставит его и он исполнит свое призвание задавить гидру революции которая теперь еще ужаснее в лице этого убийцы и злодея мы одни должны искупить кровь праведника на кого нам надеяться я вас спрашиваю англия с своим коммерческим духом не поймет и не может понять всю высоту души императора александра она отказалась очистить мальту она хочет видеть ищет заднюю мысль наших действий что они сказали новосильцову ничего они не поняли они не могут понять самоотвержения нашего императора который ничего не хочет для себя и всё хочет для блага мира и что они обещали ничего и что обещали и того не будет пруссия уж объявила что бонапарте непобедим и что вся европа ничего не может против него и я не верю ни в одном слове ни гарденбергу ни гаугвицу cette fameuse neutralité prussienne ce nest quun piège этот пресловутый нейтралитет пруссии только западня я верю в одного бога и в высокую судьбу нашего милого императора он спасет европу она вдруг остановилась с улыбкою насмешки над своею горячностью \n",
      "Dict quality:  0.24\n"
     ]
    }
   ],
   "source": [
    "encode_decode(test_text2, corpus, method='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "outside-reasoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "олна рд раяасо иевьтнуиды ана пемса иевьтнуиды соклс б шсеге леечжоиаз кесевды ногке пвемастсу лкевоо рлоге рд рло ляонтна пвтрануие а пенбмасо ьтклаьтнуиды чтнн йт пелнояиоо мосровсео йтятиао кбвлт хесз кеиомие з иамоге ио ечожтю\n"
     ]
    }
   ],
   "source": [
    "corp_freq = get_freq_list(corpus)\n",
    "enc_freq = get_freq_list(HIDDEN_TEXT)\n",
    "dec_dict = find_decode_dict(corp_freq, enc_freq, method='rank')\n",
    "print(encode_str(HIDDEN_TEXT, dec_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-folder",
   "metadata": {},
   "source": [
    "## Получается полный бред, но на больших текстах замена по рангам работает лучше, чем замена по вероятностям."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-motivation",
   "metadata": {},
   "source": [
    "## 2 Биграммы\n",
    "Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг:\n",
    "подсчитайте частоты биграмм (т.е. пар последовательных букв) по корпусам;\n",
    "проведите тестирование аналогично п.1, но при помощи биграмм.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "meaning-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_freq_list(text, ngram=2, use_freq=True):\n",
    "    result = []\n",
    "    c = Counter()\n",
    "    dl = len(text) - ngram + 1 #number of ngrams in text\n",
    "    for i in range(dl):\n",
    "        c.update([text[i:i + ngram]])\n",
    "    if use_freq:\n",
    "        for b, n in c.items():\n",
    "            result.append((b, n / dl))\n",
    "    else:\n",
    "        for b, n in c.items():\n",
    "            result.append((b, n))\n",
    "    return sorted(result, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def ngram_decode(text, dec_dict):\n",
    "    result = []\n",
    "    ngram = len(list(dec_dict.keys())[0])\n",
    "    for i in range(0, len(text), ngram):\n",
    "        try:\n",
    "            b = dec_dict[text[i:i + ngram]]\n",
    "            result.append(b)\n",
    "        except:\n",
    "            pass\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "occasional-bahamas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "егасрее  елеобво соннаскмоавпота стоь  бмек с рагопоннэтнето о пх о денони н т чтвя е  етоет ивистнии лариовмит ыленсегопочтогоеманое трсоамаяскруалчадао  сй  бнаа  ихоеркао адис чди к ук ра дбе изнонвоич тсвенизсаь ек и убы ооввоужинняотя ем рнея кооч ква хактьо канеиде дн сшиь уддрроты к вжие вае кряте лоалесепийдао щеих отой ебелли пнь рнерота сн м рыыйазьнтьм отя ем о сй деноовл гдо ел ми оролтеопор зз вш лрелиутнитим ан\n",
      "Вряд ли в результате получилась такая уж хорошая расшифровка, разве что если вы брали в качестве тестовых данных целые рассказы. Но и Шерлок Холмс был не так уж прост: после буквы E, которая действительно выделяется частотой, дальше он анализировал уже конкретные слова и пытался угадать, какими они могли бы быть. Я не знаю, как запрограммировать такой интуитивный анализ, так что давайте просто сделаем следующий логический шаг\n"
     ]
    }
   ],
   "source": [
    "#All together\n",
    "enc_dict = make_encode_dict(list(set(test_text1)))\n",
    "enc_text = encode_str(test_text1, enc_dict)\n",
    "corp_freq = get_ngram_freq_list(corpus)\n",
    "enc_freq = get_ngram_freq_list(enc_text)\n",
    "dec_dict = find_decode_dict(corp_freq, enc_freq, method='rank')\n",
    "print(ngram_decode(enc_text, dec_dict))\n",
    "print(test_text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-confidentiality",
   "metadata": {},
   "source": [
    "### Получилось гораздо хуже - биграмм больше, статистика набирается хуже, особенно по тестовому куску. Если при посимвольной расшифровке всегда правильно угадывались пробелы, то сейчас и с ними проблема. Правда, я использовал совсем уж наивный метод построения расшифровки."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-sacramento",
   "metadata": {},
   "source": [
    "# 3 & 4 MCMC\n",
    "Но и это ещё не всё: биграммы скорее всего тоже далеко не всегда работают. Основная часть задания — в том, как можно их улучшить:\n",
    "предложите метод обучения перестановки символов в этом задании, основанный на MCMC-сэмплировании, но по-прежнему работающий на основе статистики биграмм;\n",
    "реализуйте и протестируйте его, убедитесь, что результаты улучшились.\n",
    "\n",
    "Расшифруйте сообщение:\n",
    "დჳჵჂႨშႼႨშჂხჂჲდႨსႹႭჾႣჵისႼჰႨჂჵჂႨႲႹႧჲჂႨსႹႭჾႣჵისႼჰႨჲდႩჳჲႨჇႨႠჲႹქႹႨჳႹႹჱჶდსჂႽႨႩႹჲႹႭႼჰႨჵდქႩႹႨႲႭႹႧჂჲႣჲიႨჳႩႹႭდდႨშჳდქႹႨშႼႨშჳდႨჳხდჵႣჵჂႨႲႭႣშჂჵისႹႨჂႨႲႹჵჇႧჂჲდႨჾႣႩჳჂჾႣჵისႼჰႨჱႣჵჵႨეႣႨႲႹჳჵდხსდდႨႧდჲშდႭჲႹდႨეႣხႣსჂდႨႩჇႭჳႣႨႾႹჲႽႨႩႹსდႧსႹႨႽႨსჂႧდქႹႨსდႨႹჱდჶႣნ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "healthy-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_list_to_dict(freq_list):\n",
    "    result = {}\n",
    "    for c, n in freq_list:\n",
    "        result[c] = n\n",
    "    return result\n",
    "\n",
    "def get_dec_dict_score(text, decode_dict, corp_freq, ngram=2):\n",
    "    score = 0\n",
    "    decrypted_text = encode_str(text, decode_dict)\n",
    "    decrypted_freq = freq_list_to_dict(get_ngram_freq_list(decrypted_text, ngram=ngram, use_freq=False))\n",
    "    for k, v in decrypted_freq.items():\n",
    "        if k in corp_freq:\n",
    "            score += v * np.log(corp_freq[k])\n",
    "    return score\n",
    "\n",
    "def get_new_decode_dict(dec_dict, keep_spaces=False):\n",
    "    \"\"\"Randomly changes 2 keys. If keep_spaces - will not touch change them\"\"\"\n",
    "    keys = None\n",
    "    if keep_spaces:\n",
    "        while keys == None:\n",
    "            keys = random.sample(dec_dict.keys(), 2)\n",
    "            if dec_dict[keys[0]] == ' ' or dec_dict[keys[1]] == ' ':\n",
    "                keys = None\n",
    "    else:\n",
    "        keys = random.sample(dec_dict.keys(), 2)\n",
    "    new_dict = dec_dict.copy()\n",
    "#     print(keys)\n",
    "    new_dict[keys[0]], new_dict[keys[1]] = new_dict[keys[1]], new_dict[keys[0]]\n",
    "    return new_dict\n",
    "\n",
    "def random_coin(p):\n",
    "    if random.random() > p:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def init_dict(text, corpus):\n",
    "    \"\"\"Initial dict for MCMC based on top frequencies. Same as char decoding\"\"\"\n",
    "    result = {}\n",
    "    text_freq = get_freq_list(text)\n",
    "    corp_freq = get_freq_list(corpus)\n",
    "    for i in range(len(text_freq)):\n",
    "        result[text_freq[i][0]] = corp_freq[i][0]\n",
    "    for i in range(len(text_freq), 34):\n",
    "        result['_' * i] = corp_freq[i][0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "given-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_decode(text, corp_freq, n_iter=10000, corpus=corpus, ngram=2, print_progress=True):\n",
    "    cur_decode_dict = init_dict(text, corpus)\n",
    "    best_dict = None\n",
    "    score = -1e+40\n",
    "    for i in range(n_iter):\n",
    "        new_decode_dict = get_new_decode_dict(cur_decode_dict, keep_spaces=True)\n",
    "        cur_score = get_dec_dict_score(text, cur_decode_dict, corp_freq, ngram=ngram)\n",
    "        new_score = get_dec_dict_score(text, new_decode_dict, corp_freq, ngram=ngram)\n",
    "        acceptance_prob = min(1, np.exp(new_score - cur_score))\n",
    "#         print(cur_score, new_score)\n",
    "        if cur_score > score:\n",
    "            score = cur_score\n",
    "            best_dict = cur_decode_dict\n",
    "        if random_coin(acceptance_prob):\n",
    "            cur_decode_dict = new_decode_dict\n",
    "        if (i + 1) % 500 == 0 and print_progress:\n",
    "            print(i, ':', encode_str(text, best_dict))\n",
    "            print(score)\n",
    "    return best_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "combined-today",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 : орна бы бакасо тевшинятым ана дезса тевшинятым согрс у жселе реепщотаю гесевым нолге двезасися ргевоо броле бы бро рконина двибаняте а денузасо шиграшинятым пинн чи дерноктоо зосбовсео чикитао гуври цесю гетозте ю тазоле то епощих\n",
      "1918.7360678593677\n",
      "999 : арни бы бижиса товшенятым ини кодси товшенятым сапрс у юсоло роогцатих посовым налпо кводисеся рповаа брало бы бра ржанени квебинято и конудиса шепришенятым генн че корнажтаа дасбавсоа чежетиа пувре зосх потадто х тидало та огацей\n",
      "1938.4059222526482\n",
      "1499 : ерна бы бамале товшинятых ана досла товшинятых лепрл у флого роожцетак половых негпо двосалиля рповее брего бы бре рменина двибанято а донусале шипрашинятых жинн чи дорнемтее селбевлое чимитае пуври золк потесто к тасего те ожеций\n",
      "1969.8225871087022\n",
      "1999 : ерна бы бамале товшинятый ана содла товшинятый лепрл у злого роожчетак половый негпо сводалиля рповее брего бы бре рменина свибанято а сонудале шипрашинятый жинн фи сорнемтее делбевлое фимитае пуври холк потедто к тадего те ожечиц\n",
      "1980.254079254033\n",
      "2499 : ерна бы бамале товшинятый ана содла товшинятый лепрл у злого роожчетак половый негпо сводалиля рповее брего бы бре рменина свибанято а сонудале шипрашинятый жинн фи сорнемтее делбевлое фимитае пуври холк потедто к тадего те ожечиц\n",
      "1980.254079254033\n",
      "2999 : ерни бы бимиле товшанятый ини досли товшанятый лепрл у элого роожчетик половый негпо двосилаля рповее брего бы бре рменани двабинято и донусиле шапришанятый жанн ха дорнемтее селбевлое хаматие пувра золк потесто к тисего те ожечаю\n",
      "1981.0800154323952\n",
      "3499 : ерни бы бимиле товшанятый ини досли товшанятый лепрл у элого роожчетик половый негпо двосилаля рповее брего бы бре рменани двабинято и донусиле шапришанятый жанн ха дорнемтее селбевлое хаматие пувра золк потесто к тисего те ожечаю\n",
      "1981.0800154323952\n",
      "3999 : ерли пы пимите новшальный или дости новшальный текрт у этого роожчения котовый легко двоситать рковее прего пы пре рмелали двапильно и долусите шакришальный жалл за дорлемнее сетпевтое замание кувра ботя конесно я нисего не ожечаю\n",
      "2011.8527429626984\n",
      "4499 : ерли пы пимите новзальный или сочти новзальный текрт у этого роождения котовый легко свочитать рковее прего пы пре рмелали свапильно и солучите закризальный жалл ша сорлемнее четпевтое шамание кувра ботя конечно я ничего не ожедаю\n",
      "2015.6885423256622\n",
      "4999 : ерли пы пимите новзальный или сочти новзальный текрт у этого роождения котовый легко свочитать рковее прего пы пре рмелали свапильно и солучите закризальный жалл ха сорлемнее четпевтое хамание кувра ботя конечно я ничего не ожедаш\n",
      "2016.7765936846322\n",
      "5499 : ерли пы пимите новзальный или сочти новзальный текрт у этого роождения котовый легко свочитать рковее прего пы пре рмелали свапильно и солучите закризальный жалл ха сорлемнее четпевтое хамание кувра ботя конечно я ничего не ожедаю\n",
      "2017.0517800150983\n",
      "5999 : ерли пы пимите новзальный или сочти новзальный текрт у этого роождения котовый легко свочитать рковее прего пы пре рмелали свапильно и солучите закризальный жалл ба сорлемнее четпевтое бамание кувра хотя конечно я ничего не ожедаю\n",
      "2018.6846832299136\n",
      "6499 : ерли пы пимите новзальный или сочти новзальный текрт у этого роождения котовый легко свочитать рковее прего пы пре рмелали свапильно и солучите закризальный жалл ба сорлемнее четпевтое бамание кувра хотя конечно я ничего не ожедаю\n",
      "2018.6846832299136\n",
      "6999 : ерли пы пимите новзальный или сочти новзальный текрт у этого роождения котовый легко свочитать рковее прего пы пре рмелали свапильно и солучите закризальный жалл ба сорлемнее четпевтое бамание кувра хотя конечно я ничего не ожедаю\n",
      "2018.6846832299136\n",
      "7499 : ерли пы пимите новзальный или сочти новзальный текрт у этого роождения котовый легко свочитать рковее прего пы пре рмелали свапильно и солучите закризальный жалл ба сорлемнее четпевтое бамание кувра хотя конечно я ничего не ожедаю\n",
      "2018.6846832299136\n",
      "7999 : ерли пы пимите новзальный или сочти новзальный текрт у этого роождения котовый легко свочитать рковее прего пы пре рмелали свапильно и солучите закризальный жалл ба сорлемнее четпевтое бамание кувра хотя конечно я ничего не ожедаю\n",
      "2018.6846832299136\n",
      "8499 : ерли пы пимите новзальный или сочти новзальный текрт у этого роождения котовый легко свочитать рковее прего пы пре рмелали свапильно и солучите закризальный жалл ба сорлемнее четпевтое бамание кувра хотя конечно я ничего не ожедаю\n",
      "2018.6846832299136\n",
      "8999 : если вы вимите норзальный или почти норзальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите заксизальный далл ба послемнее четвертое бамание курса хотя конечно я ничего не одешаф\n",
      "2039.8902444510973\n",
      "9499 : если вы вимите норзальный или подти норзальный текст у этого сообщения который легко продитать скорее всего вы все смелали правильно и полудите заксизальный балл ча послемнее детвертое чамание курса хотя конедно я нидего не обещаю\n",
      "2041.698349971555\n",
      "9999 : если вы вимите норзальный или подти норзальный текст у этого сообщения который легко продитать скорее всего вы все смелали правильно и полудите заксизальный балл ча послемнее детвертое чамание курса хотя конедно я нидего не обещаж\n",
      "2041.9906477099323\n",
      "10499 : если вы вимите норзальный или подти норзальный текст у этого сообщения который легко продитать скорее всего вы все смелали правильно и полудите заксизальный балл ча послемнее детвертое чамание курса хотя конедно я нидего не обещаж\n",
      "2041.9906477099323\n",
      "10999 : если вы вимите норчальный или подти норчальный текст у этого сообщения который легко продитать скорее всего вы все смелали правильно и полудите чаксичальный балл за послемнее детвертое замание курса хотя конедно я нидего не обещаш\n",
      "2042.2538182646294\n",
      "11499 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "11999 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "12499 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "12999 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "13499 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "13999 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "14499 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "14999 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "15499 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "15999 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "16499 : если вы вимите норжальный или почти норжальный текст у этого соодшения который легко прочитать скорее всего вы все смелали правильно и получите жаксижальный далл за послемнее четвертое замание курса ботя конечно я ничего не одешах\n",
      "2043.4868859009352\n",
      "16999 : если вы вимите нордальный или почти нордальный текст у этого сообщения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный балл за послемнее четвертое замание курса хотя конечно я ничего не обещаю\n",
      "2045.3469205703875\n",
      "17499 : если вы вимите нордальный или почти нордальный текст у этого сообщения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный балл за послемнее четвертое замание курса хотя конечно я ничего не обещаж\n",
      "2045.6392183087648\n",
      "17999 : если вы вимите нордальный или почти нордальный текст у этого сообщения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный балл за послемнее четвертое замание курса хотя конечно я ничего не обещаж\n",
      "2045.6392183087648\n",
      "18499 : если вы вимите нордальный или почти нордальный текст у этого сообщения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный балл за послемнее четвертое замание курса хотя конечно я ничего не обещаж\n",
      "2045.6392183087648\n",
      "18999 : если вы вимите нордальный или почти нордальный текст у этого сообщения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный балл за послемнее четвертое замание курса хотя конечно я ничего не обещаж\n",
      "2045.6392183087648\n",
      "19499 : если вы вимите нордальный или почти нордальный текст у этого сообщения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный балл за послемнее четвертое замание курса хотя конечно я ничего не обещаж\n",
      "2045.6392183087648\n",
      "19999 : если вы вимите нордальный или почти нордальный текст у этого сообщения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный балл за послемнее четвертое замание курса хотя конечно я ничего не обещаж\n",
      "2045.6392183087648\n"
     ]
    }
   ],
   "source": [
    "corp_freq = get_ngram_freq_list(corpus, ngram=2, use_freq=False)\n",
    "corp_freq_dict = freq_list_to_dict(corp_freq)\n",
    "best_dict = mcmc_decode(HIDDEN_TEXT, corp_freq_dict, 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-address",
   "metadata": {},
   "source": [
    "### Получилось вполне пристойно. Перепутаны буквы М и Д, проблема с Ю - но она одна в нашем тексте и идет последней. С учетом короткости выборки - нормальный результат. Тем не менее, у функции много локальных максмумов и часто она застревает совсем не там. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-tattoo",
   "metadata": {},
   "source": [
    "# 5 Бонус: а что если от биграмм перейти к триграммам (тройкам букв) или даже больше? Улучшатся ли результаты? Когда улучшатся, а когда нет? Чтобы ответить на этот вопрос эмпирически, уже может понадобиться погенерировать много тестовых перестановок и последить за метриками, глазами может быть и не видно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "australian-jesus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ngram =  2\n",
      "если вы вимите нордальный или почти нордальный текст у этого сообщения который легко прочитать скорее всего вы все смелали правильно и получите даксидальный балл за послемнее четвертое замание курса хотя конечно я ничего не обещаж\n",
      "For ngram =  3\n",
      "если вы видите нормальный или почти нормальный текст у этого сообщения который легко прочитать скорее всего вы все сделали правильно и получите максимальный балл за последнее четвертое задание курса хотя конечно я ничего не обещаю\n",
      "For ngram =  4\n",
      "если вы видите нормальный или почти нормальный текст у этого сообщения который легко прочитать скорее всего вы все сделали правильно и получите максимальный балл за последнее четвертое задание курса хотя конечно я ничего не обещаю\n",
      "For ngram =  5\n",
      "ирла бы багами нескольный ала вечма нескольный мижрм у эмете реедхиная жемесый литже всечамомь ржесии брите бы бри ргилола всобальне а велучами кожракольный долл по верлигнии чимбисмеи погонаи жусро земя женичне я начите ни едихоe\n",
      "For ngram =  6\n",
      "инва км казали чтобывжчму ава птяла чтобывжчму лиенл ф eлтдт нттьщичар етлтому видет потяалылж нетоии книдт км кни нзивыва поыкавжчт а птвфяали быенабывжчму ьывв гы птнвизчии яилкиолти гызычаи ефоны стлр етчиячт р чаяидт чи тьищыц\n",
      "For ngram =  7\n",
      "рсак щг щклкпр еучтнашего как будпк еучтнашего присп я ьпуму суузхрекж иупучго армиу бчудкпнпш сиучрр щсрму щг щср слранак бчнщкашеу к буаядкпр тнисктнашего знаа фн бусарлерр дрпщрчпур фнлнекр иячсн 1упж иуердеу ж екдрму ер узрхнц\n",
      "For ngram =  8\n",
      "пцшт зщ зтатлп усчжьшeущг тшт фсюлт усчжьшeущг лпоцл й блсвс цсс1эпути ослсчщг шпвос фчсютльлe цосчпп зцпвс зщ зцп цапшьшт фчьзтшeус т фсшйютлп жьоцтжьшeущг 1ьшш мь фсцшпаупп юплзпчлсп мьаьутп ойчць если осупюус и утюпвс уп с1пэьн\n",
      "For ngram =  9\n",
      "олна рд раяасо иевьтнуиды ана пемса иевьтнуиды соклс б шсеге леечжоиаз кесевды ногке пвемастсу лкевоо рлоге рд рло ляонтна пвтрануие а пенбмасо ьтклаьтнуиды чтнн йт пелнояиоо мосровсео йтятиао кбвлт хесз кеиомие з иамоге ио ечожтю\n",
      "For ngram =  10\n",
      "олна рд раяасо иевьтнуиды ана пемса иевьтнуиды соклс б шсеге леечжоиаз кесевды ногке пвемастсу лкевоо рлоге рд рло ляонтна пвтрануие а пенбмасо ьтклаьтнуиды чтнн йт пелнояиоо мосровсео йтятиао кбвлт хесз кеиомие з иамоге ио ечожтю\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 11):\n",
    "    corp_freq = get_ngram_freq_list(corpus, ngram=i, use_freq=False)\n",
    "    corp_freq_dict = freq_list_to_dict(corp_freq)\n",
    "    best_dict = mcmc_decode(HIDDEN_TEXT, corp_freq_dict, 20000, ngram=i, print_progress=False)\n",
    "    print('For ngram = ', i)\n",
    "    print(encode_str(HIDDEN_TEXT, best_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-appointment",
   "metadata": {},
   "source": [
    "### По ощущениям, 3-5 граммы работают лучше и стабильнее биграмм - реже попадают в неправильные максимумы, разбираются с редкими и близкими буквами (см. М - Д и Ю в конце). При увеличении размера все ломается из-за недостаточности статистики. При размере 6 еще можно получить нормальный результат, при большей длине мне ни разу не удалось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-highway",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
